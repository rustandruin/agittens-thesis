%!TEX root = thesis.tex

\chapter{Bounds for all eigenvalues of sums of Hermitian random matrices}
\label{ch1}

\section{Introduction}
\label{ch1:sec:intro}

The classical tools of nonasymptotic random matrix theory can sometimes
give quite sharp estimates of the extreme eigenvalues of a Hermitian random matrix, but
they are not readily adapted to the study of the interior eigenvalues.
This is because, while the extremal eigenvalues are the maxima and minima of a random
process, more delicate and challenging minimax problems must be solved to obtain
the interior eigenvalues.

This chapter introduces a simple method, based upon the variational
characterization of eigenvalues, that parlays bounds on the extreme eigenvalues
of sums of random Hermitian matrices into bounds that apply to all the
eigenvalues\footnote{The content of this chapter is adapted from the technical report~\cite{GT09} co-authored with Joel Tropp.}. 
This technique extends the matrix Laplace transform method
detailed in~\cite{T10a}. 
We combine these ideas to extend several of the
inequalities in~\cite{T10a} to address the fluctuations of interior
eigenvalues. Specifically, we provide eigenvalue analogs of the classical
multiplicative Chernoff bounds
and Bennett and Bernstein inequalities.

In this technique, the delicacy of the minimax problems which implicitly define the eigenvalues
of Hermitian matrices is encapsulated in terms that reflect the fluctuations of the summands in the
appropriate eigenspaces. In particular, we see that the fluctuations of the 
$k$th eigenvalue of the sum above and below the $k$th eigenvalue of the expected 
sum are controlled by two different quantities. This satisfies intuition: for instance,
given samples from a nondegenerate stationary random process with finite covariance 
matrix, one expects that the smallest eigenvalue of the sample covariance matrix is more
likely to be an underestimate of the smallest eigenvalue of the covariance matrix
than it is to be an overestimate.

We provide two illustrative applications of our eigenvalue tail bounds: 
Theorem~\ref{ch1:thm:colsampling} quantifies the behavior of the singular values of 
matrices obtained by sampling columns from a short, fat matrix; and Theorem~\ref{ch1:thm:covarest}
quantifies the convergence of the eigenvalues of Wishart matrices.

\section{Notation}
\label{ch1:sec:notation}

 We define $\samats{n}$ to be the set of Hermitian matrices with 
 dimension $n.$ We often compare Hermitian matrices using the semidefinite ordering. 
 In this ordering, $\mat{A}$ is greater than or equal to $\mat{B}$, written 
 $\mat{A} \succeq \mat{B}$ or $\mat{B} \preceq \mat{A},$ when 
 $\mat{A} - \mat{B}$ is positive semidefinite. 


 The eigenvalues of a matrix $\mat{A}$ in $\samats{n}$ are arranged in weakly decreasing order: 
 $\lambdamax{\mat{A}} = \lambda_1(\mat{A}) \geq \lambda_2(\mat{A}) \geq \cdots \geq \lambda_n(\mat{A}) = \lambdamin{\mat{A}}.$ 
 Likewise, the singular values of a rectangular matrix $\mat{A}$ with rank $\rho$ are ordered 
 $\s_{\max}(\mat{A}) = \s_1(\mat{A}) \geq \s_2(\mat{A}) \geq \cdots \geq \s_\rho(\mat{A}) = \sigma_{\min}(\matA).$ 
 The spectral norm of a matrix $\mat{B}$ is written
 $\snorm{\mat{B}}.$
 
 
\section{The Courant--Fisher Theorem}
\label{ch1:sec:background}
In this chapter, we work over the complex field $\Cfield$. One of our central tools is the variational 
characterization of the eigenvalues of a Hermitian matrix given by the Courant--Fischer Theorem. 
For integers $d$ and $n$ satisfying $1 \leq d \leq n$, the complex Stiefel
manifold 
\[\Isom{d}{n} = \{\mat{V} \in \Cfield^{n \times d} \,:\, \mat{V}^\star \mat{V} =
\mathbf{I} \}
\]
is the collection of orthonormal bases for the $d$-dimensional subspaces of
$\Cfield^n,$ or, equivalently, the collection of all isometric embeddings of $\Cfield^d$
into $\Cfield^n.$ Let $\mat{A}$ be a Hermitian matrix with dimension $n,$ and let
$\mat{V} \in \Isom{d}{n}$ be an orthonormal basis for a subspace of $\Cfield^n.$ Then
the matrix $\mat{V}^\star \mat{A} \mat{V}$ can be interpreted as the compression
of $\mat{A}$ to the space spanned by $\mat{V}.$

\begin{prop}[Courant--Fischer (\protect{\cite[Theorem 4.2.11]{HJ85}})]
\label{ch1:prop:isometrycf}
Let $\mat{A}$ be a Hermitian matrix with dimension $n$. Then
\begin{align}
\lambda_k(\mat{A}) & = \min_{\mat{V} \in \Isom{n-k+1}{n}}
\lambdamax{\mat{V}^\star\mat{AV}} \quad \text{and}
\label{ch1:eqn:maxvariation} \\
\lambda_k(\mat{A}) & = \max_{\mat{V} \in \Isom{k}{n}} \lambdamin{\mat{V}^\star
\mat{AV}}. \label{ch1:eqn:minvariation}
\end{align}
A matrix $\mat{V}_- \in \Isom{k}{n}$ achieves equality
in~\eqref{ch1:eqn:minvariation} if and only if its columns span a top
$k$-dimensional invariant subspace of $\mat{A}.$ Likewise, a matrix $\mat{V}_+
\in \Isom{n-k+1}{n}$ achieves equality in~\eqref{ch1:eqn:maxvariation} if and only
if its columns span a bottom $(n-k+1)$-dimensional invariant subspace of
$\mat{A}$.
\end{prop}

The $\pm$ subscripts in Proposition~\ref{ch1:prop:isometrycf} are chosen to reflect
the fact that $\lambda_k(\mat{A})$ is the \emph{minimum} eigenvalue of
$\mat{V}_-^\star\mat{A}\mat{V}_-$ and the \emph{maximum} eigenvalue of
$\mat{V}_+^\star \mat{A} \mat{V}_+.$ 
As a consequence of Proposition~\ref{ch1:prop:isometrycf}, when $\mat{A}$ is
Hermitian, \mbox{$\lambda_k(-\mat{A}) = -\lambda_{n-k+1}(\mat{A}).$} This
fact allows us to use the same techniques we develop for bounding the
eigenvalues from above to bound them from below.

\section{Tail bounds for interior eigenvalues}
\label{ch1:sec:laplacetransform}
In this section we develop a generic bound on the tail probabilities of
eigenvalues of sums of independent, random, Hermitian matrices. We establish
this bound by supplementing the matrix Laplace transform methodology of~\cite{T10a} 
with Proposition~\ref{ch1:prop:isometrycf} and a result, due to Lieb
and Seiringer~\cite{LS05}, on the concavity of a certain trace function on the
cone of positive-definite matrices.

 First we observe that the Courant--Fischer Theorem allows us to relate the
behavior of the $k$th eigenvalue of a matrix to the behavior of the largest
eigenvalue of an appropriate compression of the matrix.
% NOTE:
% PINCHING -> removes interactions between subspaces
% COMPRESSION -> removes all but one subspace


% MAYBE USE A Y IN THIS STATEMENT OF THEM TO AVOID CONFUSION WHEN X = SUM X_j
% LATER
\begin{thm}
Let $\mat{Y}$ be a random Hermitian matrix with dimension $n,$ and let $k
\leq n$ be an integer. Then, for all $t \in \R,$
\begin{equation}
\Prob{\lambda_k(\mat{Y}) \geq t} \leq \inf_{\theta > 0}\,\, \min_{\mat{V} \in
\Isom{n-k+1}{n}} \left\{ \e^{-\theta t} \cdot \E\tr\e^{\theta
\mat{V}^\star\mat{YV}} \right\}.
\label{ch1:eqn:laplacetform}
\end{equation}
\label{ch1:thm:laplacetform}
\end{thm}

% JOEL PREFERS BRACES FOR EXPONENTIAL, AND USE BIG BIGG ETC TO FINER CONTROL
% THEIR SIZES
\begin{proof}
Let $\theta$ be a fixed positive number. Then
\begin{multline*}
\Prob{\lambda_k(\mat{Y}) \geq t}  = 
\Prob{\lambda_k(\theta \mat{Y}) \geq \theta t} = 
\Prob{\e^{\lambda_k(\theta \mat{Y})} \geq \e^{\theta t}} \\
\leq  \e^{-\theta t} \cdot \E \e^{\lambda_k(\theta \mat{Y})} = 
\e^{-\theta t} \cdot \E \exp\left\{\min_{\mat{V} \in \Isom{n-k+1}{n}}
\lambdamax{\theta \mat{V}^\star\mat{YV}}\right\}. 
\end{multline*}
The first identity follows from the positive homogeneity of eigenvalue maps and
the second from the monotonicity of the scalar exponential function. The final
two relations are Markov's inequality and~\eqref{ch1:eqn:maxvariation}.

To continue, we need to bound the expectation. Use monotonicity to interchange the order of the
exponential and the minimum; then apply the spectral mapping theorem to see that
\begin{align*}
 \E \exp\bigg\{\min_{\mat{V} \in \Isom{n-k+1}{n}} \lambdamax{\theta
\mat{V}^\star \mat{YV}} \bigg\} & = \E \min_{\mat{V} \in \Isom{n-k+1}{n}}
\lambdamax{\exp(\theta \mat{V}^\star\mat{YV})} \\
  & \leq \min_{\mat{V} \in \Isom{n-k+1}{n}} \E \lambdamax{\exp(\theta
\mat{V}^\star\mat{YV})} \\
  & \leq \min_{\mat{V} \in \Isom{n-k+1}{n}} \E \tr \exp(\theta
\mat{V}^\star\mat{YV}).
\end{align*}
The first inequality is Jensen's. The second inequality follows because the
exponential of a Hermitian matrix is positive definite, so its largest
eigenvalue is smaller than its trace.

Combine these observations and take the infimum over all positive $\theta$ to
complete the argument.
\end{proof}

In most cases it is prohibitively difficult to compute the quantity
$\E \tr \e^{\theta \mat{V}^\star\mat{Y V}}$ exactly. The main contribution of~\cite{T10a}
is a bound on this quantity, when $\mat{V} = \mathbf{I}$, in terms of the
cumulant generating functions of the summands. The main tool in the proof is a
classical result due to Lieb~\cite[Thm.~6]{Lieb1973} that establishes the concavity of
the function
\begin{equation}
 \mat{A} \longmapsto \trexp{\mat{H} + \log(\mat{A})} \label{ch1:eqn:tracefunctional}
\end{equation}
on the positive-definite cone, where $\mat{H}$ is Hermitian.

We are interested in the case where $\mat{V} \neq \mathbf{I}$ and the matrix $\mat{Y}$ in 
Theorem~\ref{ch1:thm:laplacetform} can be expressed as a sum of independent random matrices.
In this case, we use the following result to develop the right-hand side of the
Laplace transform bound~\eqref{ch1:eqn:laplacetform}.

%The next result gives a similar bound on $\E \trexp{\theta \mat{V}^\star
%\mat{X}\mat{V}}$ in the case of general isometric embeddings $\mat{V}.$

\begin{thm}
Consider a finite sequence $\{\mat{X}_j\}$ of independent, random, Hermitian
matrices with dimension $n$ and a sequence $\{\mat{A}_j\}$ of fixed Hermitian
matrices with dimension $n$ that satisfy the relations
\begin{equation}
 \E\e^{\mat{X}_j} \preceq \e^{\mat{A}_j}. 
 \label{ch1:eqn:logmgfdomination}
\end{equation}
Let $\mat{V} \in \Isom{k}{n}$ be an isometric embedding of $\Cfield^k$ into $\Cfield^n$
for some $k \leq n.$ Then
\begin{equation}
 \E \tr \exp\left\{\sum\nolimits_j \mat{V}^\star \mat{X}_j \mat{V} \right\} \leq
\tr \exp\left\{\sum\nolimits_j \mat{V}^\star \mat{A}_j \mat{V} \right\}.
 \label{ch1:eqn:pinchedmgf}
\end{equation}
In particular, 
\begin{equation}
 \E \tr \exp\left\{\sum\nolimits_j \mat{X}_j \right\} \leq \tr
\exp\left\{\sum\nolimits_j \mat{A}_j \right\}.
 \label{ch1:eqn:unpinchedmgf}
\end{equation}

\label{ch1:thm:mgf}
\end{thm}

%\begin{remark}
 %Since the matrix exponential is operator monotone and the matrix logarithm is
% operator monotone on $(0, \infty),$ the condition
%\[
% \E\e^{\mat{X}_j} \preceq \e^{\mat{A}_j}
%\]
%is satisfied if and only if it is also the case that 
%\[ 
%  \log \left( \E\e^{\mat{X}_j} \right) \preceq \mat{A}_j. 
%\]

%Because of this latter relation, we call $\mat{A}_j$ a semidefinite bound on
% the cumulant generating function of $\mat{X}_j.$
%\end{remark}

%Tropp's original bound on the moment generating function is an immediate
% corollary of Theorem \ref{ch1:thm:mgf}. 

%\begin{cor}
% Consider a finite sequence $\{\mat{X}_j\}$ of independent, random,
% Hermitian matrices and a sequence $\{\mat{A}_j\}$ of fixed Hermitian
%matrices that satisfy the relations
%\begin{equation*}
% \E\e^{\mat{X}_j} \preceq \e^{\mat{A}_j}.
%\end{equation*}
%Then
%\[
%  \E \trexp{\sum\nolimits_j \mat{X}_j } \leq \trexp{\sum \mat{A}_j}. 
% \]
% \label{cor:troppmgf}
% \end{cor}

Theorem~\ref{ch1:thm:mgf} is an extension of Lemma~3.4 of~\cite{T10a}, which
establishes the special case~\eqref{ch1:eqn:unpinchedmgf}. The proof depends upon a
result due to Lieb and Seiringer~\cite[Thm.~3]{LS05} that extends Lieb's
earlier result~\eqref{ch1:eqn:tracefunctional} by showing that the functional remains
concave when the $\log(\mat{A})$ term is compressed.

\begin{prop}[Lieb--Seiringer 2005]
Let $\mat{H}$ be a Hermitian matrix with dimension $k.$ Let $\mat{V} \in
\Isom{k}{n}$ be an isometric embedding of $\Cfield^k$ into $\Cfield^n$ for some $k \leq
n.$ Then the function
\begin{equation*}
 \mat{A} \longmapsto \trexp{\mat{H} + \mat{V}^\star (\log\mat{A}) \mat{V}}
\end{equation*}
is concave on the cone of positive-definite matrices in \samats{n}.
 \label{ch1:prop:pinchedtracefunctional}
\end{prop}

\begin{proof}[Proof of Theorem~\ref{ch1:thm:mgf}]
 First, note that~\eqref{ch1:eqn:logmgfdomination} and the operator monotonicity of
the matrix logarithm yield the following inequality for each $k$: 
\begin{equation}
\log \E \e^{\mat{X}_k} \preceq \mat{A}_k.
\label{ch1:eqn:logdom}
\end{equation}
Let $\E_k$ denote expectation conditioned on the first $k$ summands, $\mat{X}_1$
through $\mat{X}_k.$ Then
\begin{align*}
 \E \trexp{\sum_{j \leq \ell} \mat{V}^\star \mat{X}_j \mat{V}} & = \E\E_1\cdots
\E_{\ell-1} \trexp{\sum_{j \leq \ell-1} \mat{V}^\star \mat{X}_j \mat{V} +
\mat{V}^\star \left( \log \e^{\mat{X}_\ell} \right) \mat{V}} \\
 & \leq \E\E_1\cdots \E_{\ell-2} \trexp{\sum_{j \leq \ell-1} \mat{V}^\star
\mat{X}_j \mat{V} + \mat{V}^\star \left( \log\E\e^{\mat{X}_\ell} \right)
\mat{V}} \\
 & \leq \E\E_1\cdots \E_{\ell-2} \trexp{\sum_{j \leq \ell-1} \mat{V}^\star
\mat{X}_j \mat{V} + \mat{V}^\star \left( \log\e^{\mat{A}_\ell} \right) \mat{V} }
\\
 & = \E\E_1\cdots \E_{\ell-2} \trexp{\sum_{j \leq \ell-1} \mat{V}^\star
\mat{X}_j \mat{V} + \mat{V}^\star \mat{A}_\ell \mat{V} }.
\end{align*}
The first inequality follows from Proposition~\ref{ch1:prop:pinchedtracefunctional}
and Jensen's inequality, and the second depends on~\eqref{ch1:eqn:logdom} and the
monotonicity of the trace exponential. Iterate this argument to complete the
proof.
\end{proof}


Our main result follows from combining Theorem~\ref{ch1:thm:laplacetform} and
Theorem~\ref{ch1:thm:mgf}.

\begin{thm}[Minimax Laplace Transform]
Consider a finite sequence $\{\mat{X}_j\}$ of independent, random, Hermitian
matrices with dimension $n$, and let $k\leq n$ be an integer.
\begin{enumerate}[(i)]
 \item Let $\{\mat{A}_j\}$ be a sequence of Hermitian matrices that satisfy
the semidefinite relations
\[
 \E\e^{\theta \mat{X}_j} \preceq \e^{g(\theta) \mat{A}_j}
\]
where $g : (0,\infty) \rightarrow [0, \infty).$ Then, for all $t \in \R,$ 
\[
 \Prob{\lambda_k\left(\sum\nolimits_j \mat{X}_j \right) \geq t } \leq
\inf_{\theta > 0}\; \min_{\mat{V} \in \Isom{n-k+1}{n}} \bigg[ \e^{-\theta t}
\cdot \tr \exp\left\{ g(\theta) \sum\nolimits_j \mat{V}^\star \mat{A}_j
\mat{V}\right\}\bigg].
\]
 \label{ch1:eqn:uncompressedeigtails}
\item Let $\{\mat{A}_j:\Isom{n-k+1}{n} \rightarrow \samats{n} \}$ be a sequence
of functions that satisfy the semidefinite relations 
\[
 \E\e^{\theta\mat{V}^\star \mat{X}_j \mat{V}} \preceq \e^{g(\theta)
\mat{A}_j(\mat{V})}
\]
for all $\mat{V} \in \Isom{n-k+1}{n},$ where $g : (0, \infty) \rightarrow [0,
\infty).$ Then, for all~$t \in \R,$
\[
 \Prob{\lambda_k\left(\sum\nolimits_j \mat{X}_j \right) \geq t } \leq
\inf_{\theta > 0 } \; \min_{\mat{V} \in \Isom{n-k+1}{n}} \bigg[ \e^{-\theta t}
\cdot \tr \exp\left\{ g(\theta) \sum\nolimits_j \mat{A}_j(\mat{V})
\right\}\bigg].
\]
 \label{ch1:eqn:compressedeigtails}
\end{enumerate}

\label{ch1:thm:eigtails}
\end{thm}

The first bound in Theorem~\ref{ch1:thm:eigtails} requires less detailed information
on how compression affects the summands but correspondingly does not give as
sharp results as the second. For most cases we consider, we use the second
 inequality because it is straightforward to obtain semidefinite bounds for the
 compressed summands. The exception occurs in the proof of the subexponential
 Bernstein inequality (Theorem~\ref{ch1:thm:subexponentialbernstein} in 
 Section~\ref{ch1:sec:bernsteinbounds}); here we use the first bound, because in this case
 there are no nontrivial semidefinite bounds for the compressed summands.

In the following two sections, we use the minimax Laplace transform method to
derive Chernoff and Bernstein inequalities for the interior eigenvalues of a sum
of independent random matrices. Tail bounds for the eigenvalues of matrix
Rademacher and Gaussian series, eigenvalue Hoeffding, and matrix martingale
eigenvalue tail bounds can all be derived in a similar manner; see~\cite{T10a}
for the details of the arguments leading to such tail bounds for the maximum
eigenvalue.


\section{Chernoff bounds}
\label{ch1:sec:chernoffbounds}

Classical Chernoff bounds establish that the tails of a sum of independent
nonnegative random variables decay subexponentially. \cite{T10a}~develops
Chernoff bounds for the maximum and minimum eigenvalues of a sum of independent
positive semidefinite matrices. We extend this analysis to study the interior
eigenvalues. 

Intuitively, the eigenvalue tail bounds should depend on how concentrated the
summands are; e.g., the maximum eigenvalue of a sum of operators whose ranges
are aligned is likely to vary more than that of a sum of operators whose ranges
are orthogonal. To measure how much a finite sequence of random summands
$\{\mat{X}_j\}$ concentrates in a given subspace, we define a function $\randcon
: \bigcup_{1 \leq k \leq n} \Isom{k}{n} \rightarrow \R$ that satisfies
\begin{equation}
	\max\nolimits_j \lambdamax{\mat{V}^\star \mat{X}_j \mat{V}} \leq
\randcon(\mat{V}) \qquad \text{ almost surely for each } \mat{V} \in \bigcup_{1
\leq k \leq n} \Isom{k}{n}. 
\label{ch1:eqn:randcondef}
\end{equation}
The sequence $\{\mat{X}_j\}$ associated with $\randcon$ will always be clear
from context. We have the following result.
\begin{thm}[Eigenvalue Chernoff Bounds]
Consider a finite sequence $\{\mat{X}_j\}$ of independent, random,
positive-semidefinite matrices with dimension $n.$ Given an integer $k \leq n$,
define 
\[
\mu_k = \lambda_k\left(\sum\nolimits_j \E \mat{X}_j\right),
\]
and let $\mat{V}_{+} \in \Isom{n-k+1}{n}$ and $\mat{V}_{-} \in \Isom{k}{n}$ be
isometric embeddings that satisfy 
 $$ \mu_k = \lambdamax{\sum\nolimits_j \mat{V}_{+}^\star (\E
\mat{X}_j)\mat{V}_{+}} = \lambdamin{\sum\nolimits_j \mat{V}_{-}^\star (\E
\mat{X}_j)\mat{V}_{-}}. $$

Then 
\begin{align*}
\Prob{\lambda_k\left( \sum\nolimits_j \mat{X}_j \right) \geq (1+\delta)\mu_k} &
\leq (n-k+1) \cdot \left[\frac{\e^\delta}{(1+\delta)^{1+\delta}}
\right]^{\mu_k/\randcon(\mat{V}_{+})} 
 & \text{for } \delta > 0, \text{ and} \\ 
\Prob{\lambda_k\left(\sum\nolimits_j \mat{X}_j \right) \leq (1-\delta)\mu_k} &
\leq k \cdot
\left[\frac{\e^{-\delta}}{(1-\delta)^{1-\delta}}\right]^{\mu_k/\randcon(\mat{V}_
{-})} &  \text{for } \delta \in [0,1),
\end{align*}
where $\randcon$ is a function that satisfies~\eqref{ch1:eqn:randcondef}.
\label{ch1:thm:chernoff}
\end{thm}

Theorem~\ref{ch1:thm:chernoff} tells us how the tails of the $k$th eigenvalue are
controlled by the variation of the random summands in the top and bottom
invariant subspaces of $\sum_j \E \mat{X}_j.$ Up to the dimensional factors $k$
and $n-k+1$, the eigenvalues exhibit binomial-type tails. When $k=1$
(respectively, $k=n$) Theorem~\ref{ch1:thm:chernoff} controls the probability that
the largest eigenvalue of the sum is small (respectively, the probability that
the smallest eigenvalue of the sum is large), thereby complementing the
one-sided Chernoff bounds of~\cite{T10a}.

\begin{remark}
The results in Theorem~\ref{ch1:thm:chernoff} have the following standard
 simplifications:
  \begin{align*}
  \Prob{\lambda_k\left(\sum\nolimits_j \mat{X}_j \right) \geq t\mu_k} & \leq
 (n-k+1) \cdot \left[ \frac{\e}{t} \right]^{t \mu_k/\randcon(\mat{V}_+)} \quad
 \text{ for } t\geq \e,\text{ and} \\
  \Prob{\lambda_k\left(\sum\nolimits_j \mat{X}_j \right) \leq t\mu_k} & \leq k
 \cdot \e^{-(1-t)^2\mu_k/(2 \randcon(\mat{V}_-))} \quad \text{ for } t \in [0,1].
 \end{align*}
\end{remark}
%\todo[inline]{Prove}

\begin{remark}
If it is difficult to estimate $\randcon(\mat{V}_+)$ or $\randcon(\mat{V}_{-})$
and the summands are uniformly bounded, one can resort to the weaker estimates
\begin{align*}
	\randcon(\mat{V}_{+}) & \leq \max_{\mat{V} \in \Isom{n-k+1}{n}}
\max\nolimits_j \norm{\mat{V}^\star \mat{X}_j \mat{V}} = \max\nolimits_j
\norm{\mat{X}_j}\text{ and} \\
	\randcon(\mat{V}_{-}) & \leq \max_{\mat{V} \in \Isom{k}{n}}
\max\nolimits_j \norm{\mat{V}^\star \mat{X}_j \mat{V}} = \max\nolimits_j
\norm{\mat{X}_j}.
\end{align*}
\end{remark}

Theorem~\ref{ch1:thm:chernoff} follows from Theorem~\ref{ch1:thm:eigtails} using an
appropriate bound on the matrix moment-generating functions. The following lemma
is due to Ahlswede and Winter~\cite{AW02}; see also~\cite[Lem.~5.8]{T10a}.

\begin{lemma}
Suppose that $\mat{X}$ is a random positive-semidefinite matrix that satisfies
$\lambdamax{\mat{X}} \leq 1.$ Then
$$ \E \e^{\theta \mat{X}} \preceq \exp\left( (\e^\theta - 1) (\E\mat{X}) \right)
\quad \text{ for } \theta \in \R. $$
\label{ch1:lemma:chernoffmgf}
\end{lemma}

\begin{proof}[Proof of Theorem~\ref{ch1:thm:chernoff}, upper bound]
We consider the case where $\randcon(\mat{V}_{+}) = 1;$ the general case follows
by homogeneity.
Define 
$$ \mat{A}_j(\mat{V}_{+}) = \mat{V}_{+}^\star (\E \mat{X}_j) \mat{V}_{+} \quad
\text{and}\quad  g(\theta) = \e^\theta - 1. $$
Theorem~\ref{ch1:thm:eigtails}\eqref{ch1:eqn:compressedeigtails} and 
Lemma~\ref{ch1:lemma:chernoffmgf} imply that 
\[
\Prob{\lambda_k\left(\sum\nolimits_j \mat{X}_j\right) \geq (1+\delta)\mu_k} \leq
\inf_{\theta > 0} \e^{-\theta (1 + \delta)\mu_k} \cdot \tr\exp \left\{g(\theta)
\sum\nolimits_j\mat{V}_{+}^\star (\E \mat{X}_j) \mat{V}_{+} \right\}. 
\]
Bound the trace by the maximum eigenvalue, taking into account the reduced
dimension of the summands:
\begin{align*}
\tr\exp \left\{g(\theta) \sum\nolimits_j \mat{V}_{+}^\star(\E
\mat{X}_j)\mat{V}_{+}\right\} & \leq (n-k+1)\cdot 
\lambdamax{\exp\left\{g(\theta) \sum\nolimits_j \mat{V}_{+}^\star(\E
\mat{X}_j)\mat{V}_{+}\right\}} \\
 & = (n-k+1) \cdot \exp\left\{g(\theta) \cdot
\lambdamax{\sum\nolimits_j\mat{V}_{+}^\star(\E\mat{X}_j)\mat{V}_{+}}\right\}.
\end{align*}
The equality follows from the spectral mapping theorem. Identify the quantity
$\mu_k$; then combine the last two inequalities to obtain
\[
 \Prob{\lambda_k\left(\sum\nolimits_j \mat{X}_j \right) \geq (1 + \delta)\mu_k}
\leq \\
(n-k+1) \cdot \inf_{\theta >0 } \e^{[g(\theta) -\theta(1 + \delta)] \mu_k }. 
\]
The right-hand side is minimized when $\theta = \log(1+\delta),$ which gives the
desired upper tail bound.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{ch1:thm:chernoff}, lower bound]
As before, we consider the case where $\randcon(\mat{V}_{-}) = 1.$ Clearly,
\begin{equation}
 \Prob{ \lambda_k\left(\sum\nolimits_j \mat{X}_j \right) \leq (1-\delta)\mu_k }
= \Prob{ \lambda_{n-k+1} \left(\sum\nolimits_j -\mat{X}_j \right) \geq
-(1-\delta)\mu_k }.
 \label{ch1:eqn:chernoffeqn1}
\end{equation}
Apply Lemma~\ref{ch1:lemma:chernoffmgf} to see that, for $\theta > 0,$
\[
\E \e^{\theta (-\mat{V}^\star_- \mat{X}_j \mat{V}_-)} = \E
\e^{(-\theta)\mat{V}^\star_- \mat{X}_j \mat{V}_-} \preceq
\exp\big(g(\theta)\cdot \mat{V}^\star_-(-\E\mat{X}_j)\mat{V}_-\big), 
\]
where $g(\theta) = 1-\e^{-\theta}.$  
Theorem~\ref{ch1:thm:eigtails}\eqref{ch1:eqn:compressedeigtails} thus implies that the
latter probability in~\eqref{ch1:eqn:chernoffeqn1} is bounded by
\[
  \inf_{\theta > 0} \e^{\theta(1-\delta)\mu_k}\cdot \trexp{ g(\theta)
\sum\nolimits_j\mat{V}_{-}^\star (- \E\mat{X}_j) \mat{V}_{-} }.
\]
Using reasoning analogous to that in the proof of the upper bound, we justify
the first of the following inequalities:
\begin{align*}
\trexp{ g(\theta) \sum\nolimits_j\mat{V}_{-}^\star (-\E \mat{X}_j) \mat{V}_{-}}
& \leq k \cdot \exp\left\{ \lambdamax{ g(\theta) 
\sum\nolimits_j\mat{V}_{-}^\star (-\E \mat{X}_j) \mat{V}_{-}} \right\} \\
& = k \cdot \exp\left\{ -g(\theta) \cdot \lambdamin{\sum\nolimits_j
\mat{V}_{-}^\star (\E \mat{X}_j) \mat{V}_{-}} \right\} \\
& = k \cdot \exp\left\{ -g(\theta) \mu_k \right\}.
\end{align*}
The remaining equalities follow from the fact that $-g(\theta) <0$ and the
definition of $\mu_k.$

This argument establishes the bound
\[
\Prob{\lambda_k\left(\sum\nolimits_j \mat{X}_j \right) \leq (1 - \delta)\mu_k}
\leq k \cdot \inf_{\theta > 0} \e^{[\theta(1 - \delta) - g(\theta)]\mu_k}.
\] 
The right-hand side is minimized when $\theta = -\log(1-\delta),$ which gives
the desired lower tail bound.
\end{proof}


\section{Bennett and Bernstein inequalities}
\label{ch1:sec:bernsteinbounds}

The classical Bennett and Bernstein inequalities use the variance or knowledge
of the moments of the summands to control the probability that a sum of
independent random variables deviates from its mean. In~\cite{T10a}, matrix
Bennett and Bernstein inequalities are developed for the extreme eigenvalues of
Hermitian random matrix sums. We establish that the interior eigenvalues
satisfy analogous inequalities.

As in the derivation of the Chernoff inequalities of Section~\ref{ch1:sec:chernoffbounds}, 
we need a measure of how concentrated the random
summands are in a given subspace. Recall that the function $\randcon :
\bigcup_{1 \leq k \leq n} \Isom{k}{n} \rightarrow \R$ satisfies
\begin{equation}
	\max\nolimits_j \lambdamax{\mat{V}^\star \mat{X}_j \mat{V}} \leq
\randcon(\mat{V}) \qquad \text{ almost surely for each } \mat{V} \in \bigcup_{1
\leq k \leq n} \Isom{k}{n}. 
\label{ch1:eqn:randcondef2}
\end{equation}
The sequence $\{\mat{X}_j\}$ associated with $\randcon$ will always be clear
from context.

\begin{thm}[Eigenvalue Bennett Inequality]
\label{ch1:thm:bennett}
Consider a finite sequence $\{\mat{X}_j\}$ of independent, random, Hermitian
matrices with dimension $n$, all of which have zero mean. Given an integer $k
\leq n$, define
\[
 \sigma_k^2 = \lambda_k\left(\sum\nolimits_j \E(\mat{X}_j^2) \right). 
\]
Choose $\mat{V}_{+}\in \Isom{n-k+1}{n}$ to satisfy
\[
 \sigma_k^2 = \lambdamax{\sum\nolimits_j
\mat{V}_{+}^\star\E(\mat{X}_j^2)\mat{V}_{+}}.
\]
Then, for all $t \geq 0,$
\begin{align}
\Prob{\lambda_k\left( \sum\nolimits_j \mat{X}_j \right) \geq t } & \leq (n-k+1)
\cdot \exp\left\{-\frac{\sigma_k^2}{\randcon(\mat{V}_{+})^2} \cdot
h\left(\frac{\randcon(\mat{V}_{+})t}{\sigma_k^2}\right) \right\} \tag{i}
\label{ch1:eqn:bennettineq} \\
 & \leq (n-k+1) \cdot \exp\left\{ \frac{-t^2/2}{\sigma_k^2 +
\randcon(\mat{V}_+)t/3} \right\} \tag{ii} \label{ch1:eqn:bernsteinineq} \\
 & \leq \begin{cases}
        (n-k+1) \cdot \exp\left\{-\tfrac{3}{8}t^2/\sigma_k^2\right\}  & \text{
for } t \leq \sigma_k^2/\randcon(\mat{V}_+) \\
        (n-k+1) \cdot \exp\left\{-\tfrac{3}{8} t/\randcon(\mat{V}_+)\right\} &
\text{ for } t \geq \sigma_k^2/\randcon(\mat{V}_+),  
        \end{cases} \tag{iii}  \label{ch1:eqn:splitbernsteinineqs} 
\end{align}
where the function $h(u) = (1+u)\log(1+u) - u$ for $u \geq 0.$ The function
$\randcon$ satisfies~\eqref{ch1:eqn:randcondef2} above.
\end{thm}

Results~\eqref{ch1:eqn:bennettineq} and~\eqref{ch1:eqn:bernsteinineq} are, respectively,
matrix analogs of the classical Bennett and Bernstein inequalities. As in the
scalar case, the Bennett inequality reflects a Poisson-type decay in the tails
of the eigenvalues. The Bernstein inequality states that small deviations from
the eigenvalues of the expected matrix are roughly normally distributed while
larger deviations are subexponential. The split Bernstein inequalities~\eqref{ch1:eqn:splitbernsteinineqs} 
make explicit the division between these two regimes.

As stated, Theorem~\ref{ch1:thm:bennett} controls the probability that the
eigenvalues of a sum are large. Using the identity
\[
\lambda_k\bigg(-\sum_j \mat{X}_j\bigg) = -\lambda_{n-k+1}\bigg(\sum_j
\mat{X}_j\bigg),
\]
Theorem~\ref{ch1:thm:bennett} can also be applied to control the probability that
eigenvalues of a sum are small. 

To prove Theorem~\ref{ch1:thm:bennett}, we use the following lemma (Lemma 6.7 in~\cite{T10a})
to control the moment-generating function of a random matrix with bounded maximum eigenvalue.
 
\begin{lemma}
Let $\mat{X}$ be a random Hermitian matrix satisfying $\E\mat{X} = \mat{0}$
and $\lambdamax{\mat{X}} \leq 1$ almost surely. Then
\[ 
\E\e^{\theta\mat{X}} \preceq \exp((\e^{\theta} - \theta - 1) \cdot
\E(\mat{X}^2)) \quad \text{for } \theta > 0. 
\]
\label{ch1:lemma:freedmanmgf}
\end{lemma}

\begin{proof}[Proof of Theorem~\ref{ch1:thm:bennett}]
% CAN'T USE THIS ARGUMENT BECAUSE MATRIX EXP IS NOT OPERATOR MONOTONE ON ANY
%INTERVAL
%Using homogeneity, we assume without loss that $\randcon(\mat{V}_+)=1.$ This
%implies that $\lambdamax{\mat{X}_j} \leq 1$ almost surely for all the summands.
%By Lemma \ref{ch1:lemma:freedmanmgf}, 
%\[ 
%\E\e^{\theta \mat{V}_{+}^\star \mat{X}_j \mat{V}_{+}} \preceq \exp\left\{
% g(\theta) \cdot \E(\mat{V}_{+}^\star\mat{X}_j\mat{V}_{+})^2 \right\}. 
%\]
%with $g(\theta) = \e^{\theta} - \theta - 1.$ In fact, since
% $\mat{V}_{+}\mat{V}_{+}^\star \preceq \mathbf{I}, $
%\[ 
%\E\left[(\mat{V}_{+}^\star \mat{X}_j \mat{V}_{+})^2\right] = 
% \E(\mat{V}_{+}^\star \mat{X}_j \mat{V}_{+}\mat{V}_{+}^\star \mat{X}_j
% \mat{V}_{+}) \preceq \mat{V}_{+}^\star \E (\mat{X}_j^2) \mat{V}_{+}, 
%\]
%so
%\[
% \E\e^{\theta \mat{V}_{+}^\star \mat{X}_j \mat{V}_{+}} \preceq \exp\left\{
% g(\theta)\cdot \mat{V}_+^\star\E(\mat{X}_j^2)\mat{V}_+ \right\}. 
%\]
Using homogeneity, we assume without loss that $\randcon(\mat{V}_+)=1.$ This
implies that $\lambdamax{\mat{X}_j} \leq 1$ almost surely for all the summands.
By Lemma~\ref{ch1:lemma:freedmanmgf},
\[
\E\e^{\theta \mat{X}_j} \preceq \exp\big( g(\theta) \cdot \E(\mat{X}_j^2) \big),
\]
with $g(\theta) = \e^\theta - \theta - 1.$ 

Theorem~\ref{ch1:thm:eigtails}\eqref{ch1:eqn:uncompressedeigtails} then implies
\begin{align*}
\Prob{\lambda_k\left(\sum\nolimits_j \mat{X}_j \right) \geq t } & \leq
\inf_{\theta > 0} \e^{-\theta t} \cdot \trexp{g(\theta) \sum\nolimits_j
\mat{V}_{+}^\star\E (\mat{X}_j^2)\mat{V}_{+}} \\
 & \leq (n-k+1) \cdot \inf_{\theta >0} \e^{-\theta t} \cdot
\lambdamax{\exp\left\{g(\theta)\sum\nolimits_j
\mat{V}_{+}^\star\E(\mat{X}_j^2)\mat{V}_{+} \right\}} \\
& = (n-k+1) \cdot \inf_{\theta > 0} e^{-\theta t} \cdot \exp\left\{ g(\theta)
\cdot \lambdamax{\sum\nolimits_j \mat{V}_{+}^\star\E(\mat{X}_j^2)\mat{V}_{+} }
\right\}.
\end{align*}
The maximum eigenvalue in this expression equals $\sigma_k^2$, thus
\[ 
\Prob{\lambda_k\left(\sum\nolimits_j \mat{X}_j \right) \geq t} \leq (n-k+1)
\cdot \inf_{\theta >0 } \e^{g(\theta) \sigma_k^2 -\theta t}.
\]
The Bennett inequality~\eqref{ch1:eqn:bennettineq} follows by substituting $\theta =
\log(1 + t/\sigma_k^2)$ into the right-hand side and simplifying.

The Bernstein inequality~\eqref{ch1:eqn:bernsteinineq} is a consequence of~\eqref{ch1:eqn:bennettineq}
and the fact that 
\[
 h(u) \geq \frac{u^2/2}{1 + u/3} \quad \text{ for } u \geq 0,
\]
which can be established by comparing derivatives.

The subgaussian and subexponential portions of the split Bernstein 
inequalities~\eqref{ch1:eqn:splitbernsteinineqs} are verified through algebraic comparisons on
the relevant intervals.
\end{proof}

Occasionally, as in the application in Section~\ref{ch1:sec:covarianceest} to the
problem of covariance matrix estimation, one desires a Bernstein-type tail bound
that applies to summands that do not have bounded maximum eigenvalues. In this
case, if the moments of the summands satisfy sufficiently strong growth
restrictions, one can extend classical scalar arguments to obtain results such
as the following Bernstein bound for subexponential matrices. 

\begin{thm}[Eigenvalue Bernstein Inequality for Subexponential Matrices]
Consider a finite sequence $\{\mat{X}_j\}$ of independent, random, Hermitian
matrices with dimension $n$, all of which satisfy the subexponential moment
growth condition
\[
\E (\mat{X}_j^m) \preceq \frac{m!}{2} B^{m-2} \mat{\Sigma}_j^2 \quad \text{ for
} m=2,3,4,\ldots,
\]
where $B$ is a positive constant and $\mat{\Sigma}_j^2$ are
positive-semidefinite matrices. Given an integer $k \leq n$, set
\[
\mu_k = \lambda_k \left( \sum\nolimits_j \E \mat{X}_j \right).
\]
Choose $\mat{V}_+ \in \Isom{n-k+1}{n}$ that satisfies
\[
\mu_k = \lambdamax{ \sum\nolimits_j \mat{V}_+^\star (\E \mat{X}_j) \mat{V}_+ },
\]
and define
\[
 \quad \sigma_k^2 = \lambdamax{ \sum\nolimits_j \mat{V}_+^\star \mat{\Sigma}_j^2
\mat{V}_+ }.
\] 
Then, for any $t \geq 0,$
\begin{align}
\Prob{\lambda_k \left( \sum\nolimits_j \mat{X}_j \right) \geq \mu_k + t } & \leq
(n-k+1) \cdot \exp\left\{ - \frac{t^2/2}{\sigma_k^2 + B t}\right\} \tag{i}
\label{ch1:eqn:subexponentialbernstein} \\
 & \leq \begin{cases}
         (n-k+1) \cdot \exp\left\{ -\tfrac{1}{4} t^2/\sigma_k^2\right\} & \text{
for } t \leq \sigma_k^2/B \\
	 (n-k+1) \cdot \exp\left\{ -\tfrac{1}{4} t/B\right\} & \text{ for } t
\geq \sigma_k^2/B.
        \end{cases} \tag{ii} \label{ch1:eqn:splitsubexponentialbernstein}
\end{align}
\label{ch1:thm:subexponentialbernstein}
\end{thm}
 
This result is an extension of~\cite[Theorem 6.2]{T10a}, which, in turn,
generalizes a classical scalar argument~\cite{DG99}.

As with the other matrix inequalities, Theorem~\ref{ch1:thm:subexponentialbernstein}
follows from an application of Theorem~\ref{ch1:thm:eigtails} and appropriate
semidefinite bounds on the moment-generating functions of the summands. Thus,
the key to the proof lies in exploiting the moment growth conditions of the
summands to majorize their moment-generating functions. The following lemma, a
trivial extension of Lemma 6.8 in~\cite{T10a}, provides what we need.

\begin{lemma}
Let $\mat{X}$ be a random Hermitian matrix satisfying the subexponential
moment growth conditions
\[
\E (\mat{X}^m) \preceq \frac{m!}{2} \mat{\Sigma}^2 \quad \text{for }
m=2,3,4,\ldots.
\]
Then, for any $\theta$ in $[0,1),$
\[
 \E \exp(\theta \mat{X}) \preceq \exp\left( \theta \E \mat{X} +
\frac{\theta^2}{2(1 - \theta)} \mat{\Sigma}^2 \right).
\]
\label{ch1:lemma:growthbernsteinmgf}
\end{lemma}

% this stuff is only relevant when the summands are positive semidefinite
%
% Lemma \ref{ch1:lemma:growthbernsteinmgf} provides a bound on the mgfs of the
% $\mat{X}_j,$ but in fact we need a bound on the mgfs of $\mat{V}_+^\star
%\mat{X}_j \mat{V}_+.$ Our next lemma bridges this gap.
% 
% \begin{lemma}
%  Consider a random positive semidefinite matrix $\mat{X}$ of order $n$ and a
%fixed $\mat{V} \in \Isom{n}{k},$ where $k$ is an integer in $[1,n].$ Then for
% any integer $m \geq 1,$
% $$
% \E (\mat{V}^\star \mat{X} \mat{V})^m \preceq \mat{V}^\star \E \mat{V}^m
% \mat{V}.
% $$
% \label{lemma:expectedpinchingdomination}
% \end{lemma}
% 
% \begin{proof}
% 
% To begin with, we introduce the notation $\mat{P}_{\mat{V}} =
% \mat{V}\mat{V}^\star$ to refer to the projection unto the span of $\mat{V}.$ 
% %and note two facts. First, because $\mat{P}_{\mat{V}} \preceq \mathbf{I},$
% for any matrix $\mat{A},$ the majorization $\mat{A} \mat{P}_{\mat{V}}
% \mat{A}^\star \preceq \mat{A} \mat{A}^\star$ holds. Second, we note that
% $\mat{P}_{\mat{V}} \mat{A} \mat{P}_{\mat{V}} \preceq \mat{A}$ for any positive
% semidefinite matrix $\mat{A}$. This is easily verified by observing that, for
% any unit norm vector $\vec{x},$ 
% %$$
% %\|\mat{A}^{1/2} \mat{P}_{\mat{V}} \vec{x} \|^2 \leq \|\mat{A}^{1/2}
% \vec{x}\|^2 \Rightarrow \vec{x}^t ( \mat{A} - \mat{P}_{\mat{V}} \mat{A}
% \mat{P}_{\mat{V}} ) \vec{x} \geq 0.
% %$$
% 
% Assume that $m=2j + 1$ is an odd integer. Then
% \begin{align*}
%  \E (\mat{V}^\star \mat{X} \mat{V})^m & =
%  \E \left\{ (\mat{V}^\star \mat{X} \mat{V})^{j-1} \mat{V}^\star \mat{X}
% \mat{P}_{\mat{V}} \mat{X} \mat{P}_{\mat{V}} \mat{X} \mat{V} (\mat{V}^\star
% \mat{X} \mat{V})^{j-1} \right\} \\
%  & \preceq \E \left\{ (\mat{V}^\star \mat{X} \mat{V})^{j-1} \mat{V}^\star
% \mat{X}^3 \mat{V} (\mat{V}^\star \mat{X} \mat{V})^{j-1}\right\} \\
%  & = \E \left\{ (\mat{V}^\star \mat{X} \mat{V})^{j-2} \mat{V}^\star \mat{X}
% \mat{P}_{\mat{V}} \mat{X}^3 \mat{P}_{\mat{V}} \mat{X} \mat{V} (\mat{V}^\star
% \mat{X} \mat{V})^{j-2}\right\} \\
%  & \preceq \E \left\{ (\mat{V}^\star \mat{X} \mat{V})^{j-2} \mat{V}^\star
% \mat{X}^5 \mat{V} (\mat{V}^\star \mat{X} \mat{V})^{j-2} \right\},
% \end{align*}
% where both majorizations follow from the fact that $\mat{P}_{\mat{V}} \mat{A}
% \mat{P}_{\mat{V}} \preceq \mat{A}$ when $\mat{A}$ is a positive semidefinite
% matrix. Continuing in this fashion, we find
% \[
%  \E(\mat{V}^\star \mat{X} \mat{V})^m \preceq \mat{V}^\star \E \mat{V}^m
% \mat{V} 
% \]
% for any odd integer $m \geq 1.$
% 
% The proof for even $m$ is similar: assume that $m = 2j.$ Then
% \begin{align}
%  \E (\mat{V}^\star \mat{X} \mat{V})^m & = 
% \E \left\{ (\mat{V}^\star \mat{X} \mat{V})^{j-1} \mat{V}^\star \mat{X}
% \mat{P}_{\mat{V}} \mat{X} \mat{V} (\mat{V}^\star \mat{X} \mat{V})^{j-1}\right\}
% \\
%  & \preceq \E \left\{(\mat{V}^\star \mat{X} \mat{V})^{j-1} \mat{V}^\star
% \mat{X}^2 \mat{V} (\mat{V}^\star \mat{X} \mat{V})^{j-1}\right\} \\
%  & = \E \left\{ (\mat{V}^\star \mat{X} \mat{V})^{j-2} \mat{V}^\star \mat{X}
% \mat{P}_{\mat{V}} \mat{X}^2 \mat{P}_{\mat{V}} \mat{X} \mat{V} (\mat{V}^\star
% \mat{X} \mat{V})^{j-2} \right\} \\
%  & \preceq \E \left\{ (\mat{V}^\star \mat{X} \mat{V})^{j-2} \mat{V}^\star
% \mat{X}^4 \mat{V} (\mat{V}^\star \mat{X} \mat{V})^{j-2} \right\}.
% \end{align}
% The first majorization is justified by the fact that $\mat{P}_{\mat{V}}
% \preceq \mathbf{I}$ and the second by the fact that $\mat{P}_{\mat{V}} \mat{A}
%\mat{P}_{\mat{V}} \preceq \mat{A}$ for any positive semidefinite matrix
% $\mat{A}.$
% Continuing in this fashion, we reach the identity
% \[
%  \E (\mat{V}^\star \mat{X} \mat{V})^m \preceq \mat{V}^\star \E\mat{X}^m
% \mat{V}
% \]
% for even $m \geq 1.$
% 
% \end{proof}

\begin{proof}[Proof of Theorem~\ref{ch1:thm:subexponentialbernstein}]

We note that $\mat{X}_j$ satisfies the growth condition
\[
 \E (\mat{X}_j^m) \preceq \frac{m!}{2} B^{m-2} \mat{\Sigma}_j^2 \quad \text{for
} m \geq 2 
\]
if and only if the scaled matrix $\mat{X}_j/B$ satisfies
\[
 \E \left(\frac{\mat{X}_j}{B}\right)^m \preceq \frac{m!}{2} \cdot
\frac{\mat{\Sigma}_j^2}{B^2} \quad \text{for } m \geq 2.
\]
Thus, by rescaling, it suffices to consider the case $B = 1.$

By Lemma~\ref{ch1:lemma:growthbernsteinmgf}, the moment-generating functions of the
summands satisfy  
\[
 \E \exp(\theta \mat{X}_j) \preceq \exp\left(\theta \E \mat{X}_j + g(\theta)
\mat{\Sigma}_j^2 \right),
\]
where $g(\theta) = \theta^2/(2 -2 \theta).$ Now we apply 
Theorem~\ref{ch1:thm:eigtails}\eqref{ch1:eqn:uncompressedeigtails}:
\begin{align*}
 \Prob{\lambda_k\left(\sum\nolimits_j \mat{X}_j\right) \geq \mu_k + t} & \leq
\inf_{\theta \in [0,1)} \e^{-\theta(\mu_k + t)} \cdot \trexp{ \theta
\sum\nolimits_j \mat{V}_+^\star (\E\mat{X}_j) \mat{V}_+ + g(\theta)
\sum\nolimits_j \mat{V}_+^\star \mat{\Sigma}_j^2 \mat{V}_+ } \\
& \leq \inf_{\theta \in [0,1)} (n -k + 1)\cdot \exp\Big\{-\theta(\mu_k + t) +
\theta \cdot \lambdamax{\sum\nolimits_j \mat{V}_+^\star (\E\mat{X}_j) \mat{V}_+}
  \\
&  \qquad {} + g(\theta) \cdot \lambdamax{\sum\nolimits_j \mat{V}_+^\star
\mat{\Sigma}_j^2 \mat{V}_+} \Big\} \\
 & = \inf_{\theta \in [0, 1)} (n-k+1)\cdot \exp\left( -\theta t + g(\theta)
\sigma_k^2 \right).
\end{align*}
To achieve the final simplification, we identified $\mu_k$ and $\sigma_k^2.$
Now, select $\theta = t/(t + \sigma_k^2).$ Then simplication gives the Bernstein
inequality~\eqref{ch1:eqn:subexponentialbernstein}. 

Algebraic comparisons on the relevant intervals yield the split Bernstein
inequalities~\eqref{ch1:eqn:splitsubexponentialbernstein}.
\end{proof}

\section{An application to column subsampling}
\label{ch1:sec:colsubsampling}

As an application of our Chernoff bounds, we examine how sampling columns from a
matrix with orthonormal rows affects the spectrum. This question has
applications in numerical linear algebra and compressed sensing. The special
cases of the maximum and minimum eigenvalues have been studied in the 
literature~\cite{Tropp08, RV07}. The limiting spectral distributions of matrices formed by
sampling columns from similarly structured matrices have also been studied: the
results of~\cite{GH09} apply to matrices formed by sampling columns from any
fixed orthogonal matrix, and~\cite{F10} studies matrices formed by sampling
columns and rows from the discrete Fourier transform matrix. 

Let $\mat{U}$ be an $n \times r$ matrix with orthonormal rows. We model the
sampling operation using a random diagonal matrix $\mat{D}$ whose entries are
independent $\text{Bern}(p)$ random variables. Then the random matrix
\begin{equation}
\widehat{\mat{U}} = \mat{U}\mat{D}
\label{ch1:eqn:colsubsampling}
\end{equation}
can be interpreted as a random column submatrix of $\mat{U}$ with an average of
$p r$ nonzero columns. Our goal is to study the behavior of the spectrum of
$\widehat{\mat{U}}.$

Recall that the decay of the Chernoff tail bounds is influenced by the
variation of the random summands when compressed to invariant subspaces of the
expected sum, as measured by $\randcon(\mat{V}).$ 
In this application, the choice of invariant subspace is arbitrary, so we choose that which gives the
smallest variations and hence the fastest decay. This gives rise to a
coherence-like quantity associated with the matrix $\mat{U}:$ 
Recall that the $j$th column of $\mat{U}$ is written $\vec{u}_j.$ Consider the
following coherence-like quantity associated with $\mat{U}:$
\begin{equation}
 \tau_k  =  \min_{\mat{V} \in \Isom{k}{n}} \max\nolimits_j \norm{\mat{V}^\star
\vec{u}_j}^2 \quad \text{for } k=1,\ldots,n. 
 \label{ch1:eqn:coherencelikequantity}
\end{equation}
There does not seem to be a simple expression for $\tau_k.$ However, by choosing
$\mat{V}^\star$ to be the restriction to an appropriate $k$-dimensional
coordinate subspace, we see that $\tau_k$ always satisfies
\[
 \tau_k \leq \min_{|I|\leq k} \max\nolimits_j \sum_{i \in I} u_{ij}^2. 
\]

The following theorem shows that the behavior of $\s_k(\widehat{\mat{U}}),$ the
$k$th singular value of $\widehat{\mat{U}},$ can be explained in terms of
$\tau_k.$
\begin{thm}[Column Subsampling of Matrices with Orthonormal Rows]
\label{ch1:thm:colsampling}
Let $\mat{U}$ be an $n \times r$ matrix with orthonormal rows, and let $p$ be a
sampling probability. Define the sampled matrix $\widehat{\mat{U}}$ according to
\eqref{ch1:eqn:colsubsampling}, and the numbers $\{\tau_k\}$ according to
\eqref{ch1:eqn:coherencelikequantity}.
Then, for each $k=1,\ldots,n,$
\begin{align*}
\Prob{\s_k(\widehat{\mat{U}}) \geq \sqrt{(1 + \delta) p}} & \leq (n-k+1) \cdot
\left[ \frac{\e^\delta}{(1+\delta)^{1+\delta}} \right]^{p/\tau_{n-k+1}} &
\text{for $\delta > 0$, and} \\
\Prob{\s_k(\widehat{\mat{U}}) \leq \sqrt{(1- \delta)p} } & \leq k \cdot \left[
\frac{\e^{-\delta}}{(1-\delta)^{1-\delta}} \right]^{p/\tau_k } & \text{for
$\delta \in [0,1).$} 
\end{align*}

\end{thm}

\begin{proof}
Observe, using~\eqref{ch1:eqn:colsubsampling}, that
\[
 \s_k(\widehat{\mat{U}})^2 = \lambda_k(\mat{U} \mat{D}^2 \mat{U}^\star ) =
\lambda_k \left( \sum_j d_j \vec{u}_j \vec{u}_j^\star \right),
\]
where $\vec{u}_j$ is the $j$th column of $\mat{U}$ and $d_j \sim
\text{Bern}(p).$
Compute 
\[
 \mu_k = \lambda_k\left(\sum\nolimits_j \E d_j \vec{u}_j \vec{u}_j^\star \right)
= p \cdot \lambda_k(\mat{U} \mat{U}^\star) = p \cdot \lambda_k(\mathbf{I}) = p.
\]
It follows that, for \emph{any} $\mat{V} \in \Isom{n-k+1}{n},$
\[
\lambdamax{\sum\nolimits_j \mat{V}^\star (\E d_j \vec{u}_j \vec{u}_j^\star)
\mat{V} } = p \cdot \lambdamax{\mat{V}^\star \mat{V}} = p = \mu_k,
\]
so the choice of $\mat{V}_+ \in \Isom{n-k+1}{n}$ is arbitrary. Similarly, the
choice of $\mat{V}_- \in \Isom{k}{n}$ is arbitrary. We select $\mat{V}_+$ to be
an isometric embedding that achieves $\tau_{n-k+1}$ and $\mat{V}_-$ to be an
isometric embedding that achieves $\tau_k$. Accordingly,
\begin{align*}
 \randcon(\mat{V}_+) & = \max\nolimits_j \|\mat{V}_+^* \vec{u}_j \vec{u}_j^*
\mat{V}_+\| = \max\nolimits_j \|\mat{V}_+^* \vec{u}_j\|^2 = \tau_{n-k+1}, \quad
\text{and} \\
 \randcon(\mat{V}_-) & = \max\nolimits_j \|\mat{V}_-^* \vec{u}_j \vec{u}_j^*
\mat{V}_-\| = \max\nolimits_j \|\mat{V}_-^* \vec{u}_j\|^2 = \tau_{k}.
\end{align*}
Theorem \ref{ch1:thm:chernoff} delivers the upper bound
\begin{align*}
\Prob{\s_k(\hat{\mat{U}}) \geq \sqrt{(1 + \delta) p}} & =
\Prob{\lambda_k\left(\sum_j d_j \vec{u}_j \vec{u}_j^\star \right) \geq (1 +
\delta) p} \\
& \leq (n-k+1) \cdot \left[ \frac{\e^\delta}{(1+\delta)^{1+\delta}}
\right]^{p/\tau_{n-k+1}}
\intertext{ for $\delta > 0,$ and the lower bound }
\Prob{\s_k(\hat{\mat{U}}) \leq \sqrt{(1- \delta)p} } & =
\Prob{\lambda_k\left(\sum_j d_j \vec{u}_j \vec{u}_j^\star \right) \leq (1 -
\delta) p}
 \leq k \cdot \left[ \frac{\e^{-\delta}}{(1-\delta)^{1-\delta}}
\right]^{p/\tau_k }
\end{align*}
for $\delta \in [0,1).$
\end{proof}

\begin{figure}[t!]
\centering
\includegraphics{figures/ch1/col_spars_nlogn_100}
\caption[Spectrum of a random submatrix of a unitary DFT matrix.]{%
{\sc Spectrum of a random submatrix of a unitary DFT matrix.}
The matrix $\mat{U}$ is a $10^2 \times
10^4$ submatrix of the unitary DFT matrix with dimension $10^4,$ and the
sampling probability $p = 10^{-4} \log(10^4).$ The $k$th vertical bar,
calculated using Theorem~\ref{ch1:thm:colsampling}, describes an interval containing
the median value of the $k$th singular value of the sampled matrix
$\widehat{\mat{U}}$. The black circles denote the empirical medians of the
singular values of $\widehat{\mat{U}}$, calculated from 500 trials. The gray
circles represent the singular values of $\E \widehat{\mat{U}}.$}  
\label{ch1:fig:nlogn}
\end{figure}

To illustrate the discriminatory power of these bounds, let $\mat{U}$ be an $n
\times n^2$ matrix consisting of $n$ rows of the $n^2 \times n^2$ Fourier matrix
and choose $p = (\log n)/n$ so that, on average, sampling reduces the aspect
ratio from $n$ to $\log n.$  For $n=100,$ we determine upper and lower bounds
for the median value of $\s_k(\widehat{\mat{U}})$ by numerically finding the
value of $\delta$ where the probability bounds in Theorem~\ref{ch1:thm:colsampling}
equal one-half. Figure \ref{ch1:fig:nlogn} plots the empirical median value along with
the computed interval. We see that these ranges reflect the behavior of the
singular values more faithfully than the simple estimates $\s_k(\mathbb{E}
\widehat{\mat{U}}) = p.$

\todo[inline]{Maybe add a section on applications where you know the eigenvectors
and the summands are sufficiently simple that can predict $\randcon(\matV)$ accurately.
In particular, spectrum of random graphs}

\section{Covariance estimation}
\label{ch1:sec:covarianceest}

We conclude with an extended example that illustrates how this circle of ideas
allows one to answer interesting statistical questions. Specifically, we
investigate the convergence of the individual eigenvalues of sample covariance
matrices. Our results establish conditions under which the eigenvalues can be
recovered to relative precision, and furthermore reflect the difference in 
the probabilities of the $k$th eigenvalue of the sample covariance matrix
over- or underestimating that of the covariance matrix.

Covariance estimation is a basic and ubiquitious problem that arises in signal
processing, graphical modeling, machine learning, and genomics, among other
areas. Let $\{\vec{\eta}_j\}_{j=1}^n \subset \R^p$ be i.i.d. samples drawn from
some distribution with zero mean and covariance matrix $\mat{C}.$ Define the
sample covariance matrix 
\[
 \widehat{\mat{C}}_n = \frac{1}{n} \sum_{j=1}^n \vec{\eta}_j\vec{\eta}_j^\star.
\]
An important challenge is to determine how many samples are needed to ensure
that the empirical covariance estimator has a fixed relative accuracy in the
spectral norm. That is, given a fixed $\varepsilon,$ how large must $n$ be so
that 
\begin{equation}
\label{ch1:eqn:relacccovar}
 \snorm{\widehat{\mat{C}}_n - \mat{C}} \leq \varepsilon \snorm{\mat{C}}?
\end{equation}

This estimation problem has been studied extensively. It is now known that for
distributions with a finite second moment, $\Omega(p \log p)$ samples suffice
\cite{RU99}, and for log-concave distributions, $\Omega(p)$ samples suffice
\cite{ALPT10b}. More broadly, Vershynin \cite{V10} conjectures that, for
distributions with finite fourth moment, $\Omega(p)$ samples suffice; he
establishes this result to within iterated log factors. In~\cite{SV11}, Srivastava
and Vershynin establish that $\Omega(p)$ samples suffice for distributions
which have finite $2+\varepsilon$ moments, for some $\varepsilon >0$, and satisfy
an additional regularity condition.

Inequality \eqref{ch1:eqn:relacccovar} ensures that the difference between the
$k$th eigenvalues of $\widehat{\mat{C}}_n$ and $\mat{C}$ is small, but it
requires $\asymO{p}$ samples to obtain estimates of even a few of the eigenvalues.
Specifically, letting $\kappa_\ell = \lambda_1(\mat{C})/\lambda_\ell(\mat{C}),$ 
we see that $\const{O}(\varepsilon^{-2} \kappa_\ell^2 p)$ samples are required to obtain
relative error estimates of the largest $\ell$ eigenvalues of $\mat{C}$ using the 
results of~\cite{ALPT10b, V10, SV11}. However, it is reasonable to expect that
when the spectrum of $\mat{C}$ exhibits decay and $\ell \ll p,$ far fewer than
$\const{O}(p)$ samples should suffice to ensure relative error recovery of the largest
$\ell$ eigenvalues. 

In fact, Vershynin shows this is the case when the random vector
is subgaussian: in~\cite{Ver10}, he defines the effective rank of $\mat{C}$ to be
$r = \big(\sum_{i=1}^p \lambda_i(\mat{C})\big) / \lambda_1(\mat{C})$
and uses $r$ to provide bounds of the form~\eqref{ch1:eqn:relacccovar}.
It follow from his arguments that, with high probability, 
the largest $\ell$ eigenvalues of $\mat{C}$ are estimated to relative precision when
$n = \const{O}(\varepsilon^{-2} r \kappa_\ell^2 \log p)$
samples are taken. Clearly this result is most of interest when
the effective rank is small: e.g. when $r$ is $\const{O}(1),$ we see that 
$\const{O}(\varepsilon^{-2} \kappa_\ell^2 \log p)$
samples suffice to give relative error accuracy in the largest $\ell$ eigenvalues of $\mat{C}.$
Note, however, that this result does not supply the rates of convergence
of the \emph{individual} eigenvalues, and it requires the effective rank to be small.
To the best of the author's knowledge, there are no nonasymptotic estimates of the 
relative errors of individual eigenvalues that do not require the 
assumption that $\mat{C}$ has low effective rank.

% In particular,
% condition \eqref{ch1:eqn:relacccovar} offers virtually no information on the
% spectrum of the estimated precision matrix $\widehat{\mat{C}}_n^{-1}.$ To obtain
% this information, one must measure the approximation error on the scale of
% $\lambdamin{\mat{C}}^{-1}.$ 

In this section, we derive a relative approximation bound for each eigenvalue of
$\mat{C}$.
For simplicity, we assume the samples are drawn from a
$\mathcal{N}(\vec{0}, \mat{C})$ distribution where $\mat{C}$ is full-rank, but
we expect that the arguments can be extended to cover other subgaussian distributions.

\begin{thm}
\label{ch1:thm:covarest}
Assume that $\mat{C} \in \samats{p}$ is positive definite. Let
$\{\vec{\eta}_j\}_{j=1}^n \subset \R^p$ be i.i.d. samples drawn from a
$\mathcal{N}(\vec{0}, \mat{C})$ distribution. Define 
\[
\widehat{\mat{C}}_n = \frac{1}{n} \sum\nolimits_{j=1}^n
\vec{\eta}_j\vec{\eta}_j^\star.
\]
Write $\lambda_k$ for the $k$th eigenvalue of $\mat{C}$, and write
$\hat{\lambda}_k$ for the $k$th eigenvalue of $\widehat{\mat{C}}_n.$ Then for
$k=1,\ldots,p,$
\begin{align*}
\Prob{\hat{\lambda}_k \geq \lambda_k + t} & \leq  (p-k+1) \cdot \exp\left(
\displaystyle \frac{-nt^2}{32 \lambda_k \sum_{i=k}^p \lambda_i} \right) \quad
\text{for } t \leq 4 n \lambda_k, \\
\intertext{ and }
\Prob{\hat{\lambda}_k \leq \lambda_k - t} & \leq  k \cdot \exp\left(
\displaystyle \frac{-3nt^2}{8 \lambda_1 \big(\lambda_1 + \sum_{i=1}^k \lambda_i
\big) } \right) \quad \text{for } t \leq n\big(\lambda_1 + \sum_{i=1}^k
\lambda_i\big).
\end{align*}

% If $k$ is an integer in $[1,n],$ then for all $t > 0,$
% \begin{equation*}
% \Prob{\lambda_k(\hat{\mat{C}}_n) \geq \lambda_k(\mat{C}) + t} \leq 
% (p - k + 1) \cdot \exp \left( \frac{-nt^2}{\left(\sum\nolimits_{i=k}^p
% \lambda_i \right)(16 \lambda_k + 4t)} \right) 
% \end{equation*}
% and 
% \begin{equation*}
% \Prob{\lambda_k(\hat{\mat{C}}_n) \leq \lambda_k(\mat{C}) - t} \leq k \cdot
% \exp\left(\frac{-nt^2}{2 \lambda_1 \left(\lambda_1 + \sum\nolimits_{i=1}^k
% \lambda_i + \tfrac{t}{3} \right)} \right).
% \end{equation*}
\end{thm}

The following corollary provides an answer to our question about relative error
estimates.
\begin{cor}
\label{ch1:cor:relerrcovarest}
Let $\lambda_k$ and $\hat{\lambda}_k$ be as in Theorem \ref{ch1:thm:covarest}. Then 
\begin{align*}
\Prob{\hat{\lambda}_k \geq (1 + \varepsilon) \lambda_k} & \leq 
(p-k+1) \cdot \exp\left(\frac{- c n \varepsilon^2 }{\sum\nolimits_{i=k}^p
\frac{\lambda_i}{\lambda_k}} \right) \quad \text{for } \varepsilon \leq 4 n, \\
\intertext{ and }
\Prob{\hat{\lambda}_k  \leq (1 - \varepsilon) \lambda_k} & \leq k \cdot
\exp\left( \frac{-c n \varepsilon^2}{\tfrac{\lambda_1}{\lambda_k}\big(
\sum_{i=1}^k \frac{\lambda_i}{\lambda_k} \big)}
\right) \quad \text{for } \varepsilon \in (0,1],
\end{align*}
where the constant $c$ is at least $1/32.$
\end{cor}

The first bound in Corollary \ref{ch1:cor:relerrcovarest} tells us how many samples
are needed to ensure that $\hat{\lambda}_k$ does not overestimate $\lambda_k.$
Likewise, the second bound tells us how many samples ensure that
$\hat{\lambda}_k$ does not underestimate $\lambda_k.$

%The first bound in Corollary \ref{ch1:cor:relerrcovarest} tells us how many samples
%are needed to ensure that $\hat{\lambda}_k$ does not appreciably overestimate
%$\lambda_k$. However, it is possible that with that many samples
%$\hat{\lambda}_k$ underestimates $\lambda_k$. Thus we say that this number of
%samples guarantees that $\hat{\lambda}_k$ is a \emph{robust underestimate} of
%$\lambda_k$: that is, $\hat{\lambda}_k$ is guaranteed to either be a genuine
%underestimate or to relatively overestimate $\lambda_k$ only slightly. Likewise,
%the second bound tells us how many samples ensure that $\hat{\lambda}_k$ does
%not appreciably underestimate $\lambda_k$. Since it is possible that with this
%many samples $\hat{\lambda}_k$ overestimates $\lambda_k$, we now interpret
%$\hat{\lambda}_k$ as a \emph{robust overestimate} of $\lambda_k.$ 

% Intuition suggests that one should need only a few samples to obtain an
% accurate upper bound on $\lambda_1$ and an accurate lower bound on $\lambda_p$.
% Corollary \ref{ch1:cor:relerrcovarest} quantifies this intuition, saying that
% $\mathrm{O}(1)$ samples are sufficient for $\hat{\lambda}_1$ and
% $\hat{\lambda}_p$ to be, respectively, a robust overestimate and a robust
% underestimate. The intuition that one needs a large number of samples to
% accurately upper bound $\lambda_p$ or lower bound $\lambda_1$ is also
% quantified: we see that $\mathrm{O}(p \log p)$ samples suffice. More generally,
% it follows from Corollary \ref{ch1:cor:relerrcovarest} that $\hat{\lambda}_k$ is a
% robust underestimate of $\lambda_k$ after $\mathrm{O}((p-k+1) \log (p-k+1) + 1)$
% observations and a robust overestimate after $\mathrm{O}(k \log k + 1)$
% observations. Consequently, $\mathrm{O}(p\log p)$ samples allow us to determine
% the full spectrum of $\mat{C}$ to within relative accuracy. 

Corollary \ref{ch1:cor:relerrcovarest} suggests that the relationship of
$\hat{\lambda}_k$ to $\lambda_k$ is determined by the spectrum of $\mat{C}$ in
the following manner. When the eigenvalues below $\lambda_k$ are small
compared with $\lambda_k$, the quantity
\[
 \sum_{i=k}^p \lambda_i/\lambda_k
\]
is small (viz., it is no larger than $p-k+1$), and so $\hat{\lambda}_k$ is not likely to overestimate $\lambda_k$.
Similarly, when the eigenvalues above $\lambda_k$ are comparable with
$\lambda_k$, the quantity
\[
\frac{\lambda_1}{\lambda_k}\left(\sum_{i=1}^k
\lambda_i/\lambda_k  \right)
\]
is small (viz., it is no larger than $k\cdot\kappa_k^2$), and so $\hat{\lambda}_k$ is not likely to underestimate $\lambda_k$.


% It is probably the case that a more refined analysis would find that the
%probability of overestimation is controlled by the ratio of \lambda_k to
%\lambda_{k-1} and that of underestimation is controlled by the ratio of
%\lambda_k to \lambda_{k+1}. I.e., only the adjacent eigenvalues affect lambda_k
% Experimentally, looks like the bound of (p \log p)/\epsilon^2 samples needed
% to ensure the sample covar of an indep Gaussian process has all eigenvalues
% within \pm\epsilon of 1 is sharp. look at bounds on wishart matrix evals for
% comparison (or NCK ineq)

\begin{remark}
The results in Theorem \ref{ch1:thm:covarest} and Corollary \ref{ch1:cor:relerrcovarest}
also apply when $\mat{C}$ is rank-deficient: simply replace each occurence of
the dimension $p$ in the bounds with $\rank(\mat{C}).$ 

Indeed, assume that $\mat{C}$ is rank-deficient and take its truncated
eigenvalue decomposition to be $\mat{C} = \mat{U} \mat{\Lambda} \mat{U}^\star.$
If $\vec{\eta}_j \sim \mathcal{N}(\vec{0}, \mat{C}),$ then $\vec{\eta}_j$ lies
in the span of $\mat{C}.$ It follows that $\hat{\lambda}_k = \lambda_k = 0$ for
all $k > \rank(\mat{C}).$ When $k \leq \rank(\mat{C}),$ we observe that  
\[
\lambda_k(\mat{C}) = \lambda_k(\mat{\Lambda}) \quad \text{ and } \quad
\lambda_k\left(\sum_j \vec{\eta}_j \vec{\eta}_j^\star \right) = \lambda_k\left(
\sum_j \vec{\xi}_j \vec{\xi}_j^\star \right),
\]
where $\vec{\xi}_j = \mat{U}^\star \vec{\eta}_j$ is distributed
$\mathcal{N}(\vec{0}, \mat{\Lambda}).$ Thus,
\[
\left| \lambda_k\left(\sum_j \vec{\eta}_j \vec{\eta}_j^\star \right) -
\lambda_k(\mat{C}) \right| =
\left| \lambda_k\left( \sum_j \vec{\xi}_j \vec{\xi}_j^\star \right) -
\lambda_k(\mat{\Lambda}) \right|.
\]
Consequently, the problem of estimating the eigenvalues of $\mat{C}$ to relative
error using the samples $\{\vec{\eta}_j\}$ is equivalent to that of estimating
the eigenvalues of the full-rank covariance matrix $\mat{\Lambda}$ to relative
error using the samples $\{\vec{\xi}_j\}.$
\end{remark}
% 
% 
% \begin{remark}
% From Corollary~\ref{ch1:cor:relerrcovarest}, we see that if $n = \Omega(
% \varepsilon^{-2} \max \{ (p-k+1)\log(p-k+1)), k \log k \} ),$ then with high
% probability $\hat{\lambda}_k$ estimates $\lambda_k$ to within a relative error
% of $\varepsilon.$ Observe that for each of the $p$ eigenvalues, the number of
% samples required to ensure relative error estimation is
% $\mathrm{O}(\varepsilon^{-2} p \log p).$ Theorem \ref{thm:examplecovarest} now
% follows from a union bound.
% \end{remark}
% 

It is reasonable to expect that one should be able to use Corollary~\ref{ch1:cor:relerrcovarest} 
to recover Vershynin's result in~\cite{Ver10} for Wishart 
matrices: that $\Omega(\varepsilon^{-2} r \kappa_\ell^2 \log p)$ samples suffice to estimate the 
eigenvalues of the covariance matrix of a Gaussian random variable to within a relative precision of 
$1 \pm \varepsilon.$ Indeed, this result follows
from Corollary~\ref{ch1:cor:relerrcovarest} and a simple union bound argument.

\begin{cor}
Assume $\mat{C}$ is positive semidefinite. Let $\{\vec{\eta}_j\}_{j=1}^n \subset
\R^p$ be i.i.d. samples drawn from a $\mathcal{N}(\vec{0}, \mat{C})$
distribution. If $n = \Omega(\varepsilon^{-2} r \kappa_\ell^2 \log p),$ then with high probability
\[
|\lambda_k(\widehat{\mat{C}}_n) - \lambda_k(\mat{C})| \leq \varepsilon
\lambda_k(\mat{C}) \quad \text{for } k=1, \ldots,\ell.
\]
\label{ch1:cor:examplecovarest}
\end{cor}

\begin{proof}
 From Corollary~\ref{ch1:cor:relerrcovarest}, we see that 
 \[
  \Prob{\lambda_k(\widehat{\mat{C}}_n) \leq (1-\varepsilon) \lambda_k} \leq p^{-\beta} 
  \quad \text{ when } n \geq 32 \varepsilon^{-2} 
  \left(\frac{\lambda_1}{\lambda_k} \sum\nolimits_{i \leq k} \frac{\lambda_i}{\lambda_k} \right)
  (\log k + \beta \log p).
 \]
Recall that $\kappa_k = \lambda_1(\mat{C})/\lambda_k(\mat{C})$ and 
$r = \big(\sum_{i=1}^p \lambda_i(\mat{C})\big)/\lambda_1(\mat{C}),$ so
\[
  \left(\frac{\lambda_1}{\lambda_k} \sum\nolimits_{i \leq k} 
   \frac{\lambda_i}{\lambda_k} \right) \leq \kappa_k^2 r.
\]
Clearly, taking $n = \Omega(\varepsilon^{-2} r \kappa_\ell^2 \log p)$ samples ensures that, 
with high probability, each of the top $\ell$ eigenvalues of the sample covariance matrix
satisfies $\lambda_k(\widehat{\mat{C}}_n) > (1 - \varepsilon)\lambda_k.$

Likewise,
\[
 \Prob{\lambda_k(\widehat{\mat{C}}_n) \geq (1-\varepsilon) \lambda_k} \leq p^{-\beta} 
 \quad \text{ when } n \geq 32 \varepsilon^{-2} 
 \left( \sum\nolimits_{i \geq k} \frac{\lambda_i}{\lambda_k} \right)
 (\log (p-k+1) + \beta \log p)
\]
and
\[
 \sum\nolimits_{i \geq k} \frac{\lambda_i}{\lambda_k} = \frac{\lambda_1}{\lambda_k} \frac{\left(\sum\nolimits_{i \geq k} \lambda_i\right)}{\lambda_1} \leq 
 \kappa_k \frac{\left(\sum\nolimits_{i=1}^p \lambda_i\right)}{\lambda_1} = \kappa_k r,
\]
so we see that taking $n = \Omega(\varepsilon^{-2} r \kappa_\ell \log p)$ samples ensures that, 
with high probability, each of the top $\ell$ eigenvalues of the sample covariance matrix 
satisfies $\lambda_k(\widehat{\mat{C}}_n) < (1 + \varepsilon)\lambda_k.$

Combining these two results, we conclude that $n = \Omega(\varepsilon^{-2} r \kappa_\ell^2 \log p)$ 
ensures that the top $\ell$ eigenvalues of $\mat{C}$ are estimated to within relative precision 
$1 \pm \varepsilon$ with probability at least $1 - 2\ell p^{-\beta}.$
\end{proof}

\subsection{Proof of Theorem \ref{ch1:thm:covarest}}
We now prove Theorem \ref{ch1:thm:covarest}. This result requires a number of
supporting lemmas; we defer their proofs until after a discussion of extensions
to Theorem \ref{ch1:thm:covarest}.

We study the error $|\lambda_k(\widehat{\mat{C}}_n) - \lambda_k(\mat{C})|.$ To
apply the methods developed in this chapter, we pass to a question about the
eigenvalues of a difference of two matrices. The first lemma accomplishes this
goal by compressing both the population covariance matrix and the sample
covariance matrix to a fixed invariant subspace of the population covariance
matrix.

\begin{lemma}
\label{ch1:lemma:splittail}
Let $\mat{X}$ be a random Hermitian matrix with dimension $p,$ and let
$\mat{A}$ be a fixed Hermitian matrix with dimension $p$. Choose $\mat{W}_+
\in \Isom{p-k+1}{p}$ and $\mat{W}_- \in \Isom{k}{p}$ for which
\[
  \lambda_k(\mat{A}) = \lambdamax{\mat{W}_+^*\mat{A}\mat{W}_+} =
\lambdamin{\mat{W}_-^*\mat{A}\mat{W}_-}.
\]
Then, for all $t >0,$
\begin{align}
 \Prob{\lambda_k(\mat{X}) \geq \lambda_k(\mat{A}) + t} & \leq
\Prob{\lambdamax{\mat{W}_+^\star \mat{X} \mat{W}_+} \geq \lambda_k(\mat{A}) + t
} \label{ch1:eqn:covardeterministicupperbnd}\\
\intertext{ and }
 \Prob{\lambda_k(\mat{X}) \leq \lambda_k(\mat{A}) -t } &\leq
\Prob{\lambdamax{\mat{W}_-^\star(\mat{A} - \mat{X}) \mat{W}_-} \geq t}.
 \label{ch1:eqn:covardeterministiclowerbnd}
\end{align}
\end{lemma}

We apply this result with $\mat{A} = \mat{C}$ and $\mat{X} =
\widehat{\mat{C}}_n.$ The first estimate \eqref{ch1:eqn:covardeterministicupperbnd}
and the second estimate \eqref{ch1:eqn:covardeterministiclowerbnd} are handled using
different arguments. The second estimate is easier because the maximum
eigenvalue of the matrix $\mat{C} - \widehat{\mat{C}}_n$ is bounded. Indeed,
\[
 \lambdamax{\mat{W}_+^\star (\mat{C} - \widehat{\mat{C}}_n) \mat{W}_+} \leq
\lambdamax{\mat{W}_+^\star \mat{C} \mat{W}_+}.
\]
Thus, we may use Theorem \ref{ch1:thm:bennett} to complete the second estimate. The
next lemma gives the matrix variances that we need to apply this theorem.

\begin{lemma}
\label{ch1:lemma:gaussiancentralsecondmoment}
Let $\vec{\xi} \sim \mathcal{N}(\mat{0}, \mat{G}).$ Then
$$
\E(\vec{\xi}\vec{\xi}^\star - \mat{G})^2 = \mat{G}^2 + \tr(\mat{G})\cdot
\mat{G}. 
$$
\end{lemma}

The first inequality \eqref{ch1:eqn:covardeterministicupperbnd} is harder because
$\widehat{\mat{C}}_n$ is unbounded. In this case, we may apply Theorem
\ref{ch1:thm:subexponentialbernstein}. To use this theorem, we need the following
moment growth estimate for rank-one Wishart matrices.

\begin{lemma}
\label{ch1:lemma:momentbound}
Let $\vec{\xi} \sim \mathcal{N}(\vec{0},\mat{G}).$ Then for any integer $m \geq
2,$
$$
\E\left(\vec{\xi}\vec{\xi}^\star \right)^m \preceq 2^m m! (\tr
\mat{G})^{m-1}\cdot \mat{G}.
$$
\end{lemma}

With these preliminaries addressed, we prove Theorem \ref{ch1:thm:covarest}.

\begin{proof}[Proof of lower estimate in Theorem~\ref{ch1:thm:covarest}]

First we consider the probability that $\hat{\lambda}_k$ underestimates
$\lambda_k$. Let $\mat{W}_- \in \Isom{k}{p}$ satisfy
\[
 \lambda_k(\mat{C}) = \lambdamin{\mat{W}_-^\star \mat{C} \mat{W}_-}.
\]
Then Lemma \ref{ch1:lemma:splittail} implies
\begin{align*}
 \Prob{\lambda_k(\widehat{\mat{C}}_n) \leq \lambda_k(\mat{C}) - t} & \leq
\Prob{\lambdamax{\mat{W}_-^\star (\mat{C} - \widehat{\mat{C}}_n) \mat{W}_-} \geq
t} \\
& = \Prob{ \lambdamax{\sum\nolimits_j \mat{W}_-^\star(\mat{C} -
\vec{\eta}_j\vec{\eta}_j^\star ) \mat{W}_-} \geq nt}.
\end{align*}
The factor $n$ comes from the normalization of the sample covariance matrix.
Each term in the sum is zero mean and bounded above by
$\mat{W}_-^*\mat{C}\mat{W}_-$ in the semidefinite order, so Theorem
\ref{ch1:thm:bennett} applies. As we desire a bound on the maximum eigenvalue of the
sum, we take $\mat{V}_+ = \mathbf{I}$ when we invoke Theorem \ref{ch1:thm:bennett}.
Then 
\[
 \sigma_1^2 = \lambdamax{ \sum\nolimits_j \E\left[\mat{W}_-^\star(\mat{C} -
\vec{\eta}_j\vec{\eta}_j^\star)\mat{W}_-\right]^2 } = n
\lambdamax{\E\left[\mat{W}_-^\star(\mat{C} -
\vec{\eta}_1\vec{\eta}_1^\star)\mat{W}_-\right]^2}.
\]
The covariance matrix of $\vec{\eta}_1$ is $\mat{C},$ so that of
$\mat{W}_-^\star \vec{\eta}_1$ is $\mat{W}_-^\star \mat{C} \mat{W}_-.$ It
follows from  Lemma \ref{ch1:lemma:gaussiancentralsecondmoment} that
\[
\E\left[\mat{W}_-^*(\mat{C} - \vec{\eta}_1\vec{\eta}_1^\star)\mat{W}_-\right]^2
= (\mat{W}_-^*\mat{C}\mat{W}_-)^2 + \tr(\mat{W}_-^*\mat{C}\mat{W}_-) \cdot
\mat{W}_-^*\mat{C}\mat{W}_-.
\]
Observe that $\mat{W}_-^*\mat{C}\mat{W}_-$ is the restriction of $\mat{C}$ to
its top $k$-dimensional invariant subspace, so
\[
 \sigma_1^2 = n \lambdamax{\E\left[\mat{W}_-^\star(\mat{C} -
\vec{\eta}_1\vec{\eta}_1^\star)\mat{W}_-\right]^2} = n\lambda_1(\mat{C}) \left(
\lambda_1(\mat{C}) + \sum_{i=1}^k \lambda_i(\mat{C}) \right) 
\]
and we can take $\randcon(\mat{V}_+) = \lambdamax{\mat{C}}.$

The subgaussian branch of the split Bernstein inequality of Theorem
\ref{ch1:thm:bennett} shows that 
\begin{equation*}
 \Prob{ \lambdamax{\sum\nolimits_j \mat{W}_-^\star(\mat{C} -
\vec{\eta}_j\vec{\eta}_j^\star ) \mat{W}_-} \geq nt} \leq 
 k \cdot \exp\left( \displaystyle \frac{-3nt^2}{8 \lambda_1(\mat{C})
\big(\lambda_1(\mat{C}) + \sum_{i=1}^k \lambda_i(\mat{C}) \big) } \right)
\end{equation*}
when  $t \leq n\big(\lambda_1(\mat{C}) + \sum_{i=1}^k \lambda_i(\mat{C})\big).$
This inequality provides the desired bound on the probability that
$\lambda_k(\widehat{\mat{C}}_n)$ underestimates $\lambda_k(\mat{C})$.
\end{proof}

\begin{proof}[Proof of upper estimate in Theorem~\ref{ch1:thm:covarest}]
Now we consider the probability that $\hat{\lambda}_k$ overestimates
$\lambda_k$. Let $\mat{W}_+ \in \Isom{p-k+1}{p}$ satisfy
\[
 \lambda_k(\mat{C}) = \lambdamax{\mat{W}_+^\star \mat{C} \mat{W}_+}.
\]
Then Lemma \ref{ch1:lemma:splittail} implies
\begin{align}
 \Prob{\lambda_k(\widehat{\mat{C}}_n) \geq \lambda_k(\mat{C}) + t} & \leq
\Prob{\lambdamax{\mat{W}_+^\star \widehat{\mat{C}}_n \mat{W}_+} \geq
\lambda_k(\mat{C}) + t} \notag \\
& = \Prob{\lambdamax{\sum\nolimits_j \mat{W}_+^\star (\vec{\eta}_j
\vec{\eta}_j^\star) \mat{W}_+} \geq n \lambda_k(\mat{C}) + n t}.
\label{ch1:eqn:upperestimateprob}
\end{align}
The factor $n$ comes from the normalization of the sample covariance matrix.

The covariance matrix of $\vec{\eta}_j$ is $\mat{C},$ so that of
$\mat{W}_+^\star \vec{\eta}_j$ is $\mat{W}_+^\star\mat{C}\mat{W}_+.$ Apply Lemma
\ref{ch1:lemma:momentbound} to verify that $\mat{W}_+^\star \vec{\eta}_j$ satisfies
the subexponential moment growth bound required by Theorem
\ref{ch1:thm:subexponentialbernstein} with 
\[
 B = 2 \tr(\mat{W}_+^*\mat{C}\mat{W}_+) \quad\text{ and }\quad \mat{\Sigma}_j^2
= 8\tr(\mat{W}_+^*\mat{C}\mat{W})\cdot \mat{W}_+^*\mat{C}\mat{W}_+.
\]
In fact, $\mat{W}_+^*\mat{C}\mat{W}_+$ is the compression of $\mat{C}$ to the
invariant subspace corresponding with its bottom $p-k+1$ eigenvalues, so 
\[
B = 2 \sum\nolimits_{i=k}^p \lambda_i(\mat{C}) \quad\text{and}\quad
\lambdamax{\mat{\Sigma}_j^2} = 8 \lambda_k(\mat{C}) \sum\nolimits_{i=k}^p
\lambda_i(\mat{C}).
\]
We are concerned with the maximum eigenvalue of the sum in
\eqref{ch1:eqn:upperestimateprob}, so we take $\mat{V}_+ = \mathbf{I}$ in the
statement of Theorem \ref{ch1:thm:subexponentialbernstein} to find that
\begin{align*}
 \sigma_1^2 & = \lambdamax{\sum\nolimits_j \mat{\Sigma}_j^2 } = n
\lambdamax{\mat{\Sigma}_1^2} = 8 n \lambda_k(\mat{C}) \sum\nolimits_{i=k}^p
\lambda_i(\mat{C}) \quad \text{and}
\\ \mu_1 & = \lambdamax{\sum_j \mat{W}_+^\star
\E(\vec{\eta}_j\vec{\eta}_j^\star) \mat{W}_+ } = n\lambdamax{\mat{W}_+^\star
\mat{C} \mat{W}_+} = n \lambda_k(\mat{C}).
\end{align*}
It follows from the subgaussian branch of the split Bernstein inequality of
Theorem \ref{ch1:thm:subexponentialbernstein} that 
$$
\Prob{\lambda_k \left(\sum\nolimits_j \mat{W}_+^\star (\vec{\eta}_j
\vec{\eta}_j^\star) \mat{W}_+ \right) \geq n \lambda_k(\mat{C}) + n t} \leq 
 (p-k+1) \cdot \exp\left( \displaystyle \frac{-nt^2}{32 \lambda_k(\mat{C})
\sum_{i=k}^p \lambda_i(\mat{C})} \right)
$$
when $t \leq 4 n \lambda_k(\mat{C}).$ This provides the desired bound on the
probability that $\lambda_k(\widehat{\mat{C}}_n)$ overestimates
$\lambda_k(\mat{C}).$
\end{proof}

\subsection{Extensions of Theorem \ref{ch1:thm:covarest}}
Results analogous to Theorem \ref{ch1:thm:covarest} can be established for other
distributions. If the distribution is bounded, the possibility that
$\hat{\lambda}_k$ deviates above or below $\lambda_k$ can be controlled using
the Bernstein inequality of Theorem \ref{ch1:thm:bennett}. If the distribution is
unbounded but has matrix moments that satisfy a sufficiently nice growth
condition, the probability that $\hat{\lambda}_k$ deviates below $\lambda_k$ can
be controlled with the Bernstein inequality of Theorem \ref{ch1:thm:bennett} and the
probability that it deviates above $\lambda_k$ can be bounded using a Bernstein
inequality analogous to that in Theorem \ref{ch1:thm:subexponentialbernstein}.

We established Theorem \ref{ch1:thm:covarest} using this technique to demonstrate
the simplicity of the Laplace transform machinery. However, the results of
\cite{ALPT10b} on the convergence of empirical covariance matrices of isotropic
log-concave random vectors lead to tighter bounds on the probability that
$\hat{\lambda}_k$ overestimates $\lambda_k.$ There does not seem to be an
analogous reduction for handling the probability that $\hat{\lambda}_k$ is an
underestimate.

To see the relevance of the results in \cite{ALPT10b}, first observe the
following consequence of the subadditivity of the maximum eigenvalue mapping:
\begin{align*}
\lambdamax{\mat{W}_+^\star(\mat{X} - \mat{A}) \mat{W}_+} & \geq
\lambdamax{\mat{W}_+^\star \mat{X} \mat{W}_+} - \lambdamax{\mat{W}_+^\star
\mat{A} \mat{W}_+} \\
 &= \lambdamax{\mat{W}_+^\star \mat{X} \mat{W}_+} - \lambda_k(\mat{A}).
\end{align*}
In conjunction with \eqref{ch1:eqn:covardeterministicupperbnd}, this gives us the
following control on the probability that $\lambda_k(\mat{X})$ overestimates
$\lambda_k(\mat{A}):$ 
\[
 \Prob{\lambda_k(\mat{X}) \geq \lambda_k(\mat{A}) + t} \leq
\Prob{\lambdamax{\mat{W}_+^\star (\mat{X} - \mat{A}) \mat{W}_+} \geq t}.
\]

In our application, $\mat{X}$ is the empirical covariance matrix and $\mat{A}$
is the actual covariance matrix. The spectral norm dominates the maximum
eigenvalue, so 
\begin{align*}
 \Prob{\lambda_k(\widehat{\mat{C}}_n) \geq \lambda_k(\mat{C}) + t} & \leq
\Prob{\lambdamax{\mat{W}_+^\star(\widehat{\mat{C}}_n - \mat{C})\mat{W}_+} \geq
t}\\
& \leq \Prob{\|\mat{W}_+^\star (\widehat{\mat{C}}_n - \mat{C}) \mat{W}_+\| \geq
t} = \Prob{\|\mat{W}_+^\star \widehat{\mat{C}}_n \mat{W}_+ - \mat{S}^2 \| \geq t
},
\end{align*}
where $\mat{S}$ is the square root of $\mat{W}_+^\star \mat{C} \mat{W}_+.$ Now
factor out $\mat{S}^2$ and identify $\lambda_k(\mat{C}) = \|\mat{S}^2\|$ to
obtain
\begin{align*}
 \Prob{\lambda_k(\widehat{\mat{C}}) \geq \lambda_k(\mat{C}) + t} & \leq
\Prob{\|\mat{S}^{-1} \mat{W}_+^\star \widehat{\mat{C}}_n \mat{W}_+ \mat{S}^{-1}
- \mathbf{I} \| \|\mat{S}^2\| \geq t } \\
 & = \Prob{\|\mat{S}^{-1} \mat{W}_+^\star \widehat{\mat{C}}_n \mat{W}_+
\mat{S}^{-1} - \mathbf{I} \| \geq t/\lambda_k(\mat{C})}.
\end{align*}
Note that if $\vec{\eta}$ is drawn from a $\mathcal{N}(\vec{0}, \mat{C})$
distribution, then the covariance matrix of the transformed sample
$\mat{S}^{-1}\mat{W}_+^\star \vec{\eta}$ is the identity:
\[
\E \left(\mat{S}^{-1}\mat{W}_+^\star \vec{\eta} \vec{\eta}^\star \mat{W}_+
\mat{S}^{-1}\right) = \mat{S}^{-1} \mat{W}_+^\star \mat{C} \mat{W}_+
\mat{S}^{-1} = \mathbf{I}.
\]
Thus $\mat{S}^{-1} \mat{W}_+^\star \widehat{\mat{C}}_n \mat{W}_+ \mat{S}^{-1}$
is the empirical covariance matrix of a standard Gaussian vector in
$\R^{p-k+1}.$ By Theorem 1 of \cite{ALPT10b}, it follows that $\hat{\lambda}_k$
is unlikely to overestimate $\lambda_k$ in relative error when the number $n$ of
samples is $\Omega(p-k+1).$

Similarly, for more general distributions, the bounds on the probability of
$\hat{\lambda}_k$ exceeding $\lambda_k$ can be tightened beyond those suggested
in Theorem \ref{ch1:thm:covarest} by using the results in \cite{ALPT10b} or
\cite{V10}.

Finally, we note that the techniques developed in the proof of Theorem
\ref{ch1:thm:covarest} can be used to investigate the spectrum of the error matrices
$\widehat{\mat{C}}_n - \mat{C}.$
\subsection{Proofs of the supporting lemmas}

We now establish the lemmas used in the proof of Theorem \ref{ch1:thm:covarest}.

\begin{proof}[Proof of Lemma \ref{ch1:lemma:splittail}]
The probability that $\lambda_k(\mat{X})$ overestimates $\lambda_k(\mat{A})$ is
controlled with the sequence of inequalities
\begin{align*}
\Prob{\lambda_k(\mat{X}) \geq \lambda_k(\mat{A}) + t} & = \Prob{ \inf_{\mat{W}
\in \Isom{p-k+1}{p}} \lambdamax{\mat{W}^*\mat{X}\mat{W}} \geq \lambda_k(\mat{A})
+ t} \\
& \leq \Prob{\lambdamax{\mat{W}_+^*\mat{X}\mat{W}_+} \geq \lambda_k(\mat{A}) +
t}. 
\end{align*}

We use a related approach to study the probability that $\lambda_k(\mat{X})$
underestimates $\lambda_k(\mat{A}).$ Our choice of $\mat{W}_-$ implies that 
\[
\lambda_{p-k+1}(-\mat{A}) = -\lambda_k(\mat{A}) =
-\lambdamin{\mat{W}_-^*\mat{A}\mat{W}_-} =
\lambdamax{\mat{W}_-^*(-\mat{A})\mat{W}_-}. 
\]
It follows that
\begin{align*}
\Prob{\lambda_k(\mat{X}) \leq \lambda_k(\mat{A}) - t} & =
\Prob{\lambda_{p-k+1}(-\mat{X}) \geq \lambda_{p-k+1}(-\mat{A}) + t} \\
& = \Prob{ \inf_{\mat{W} \in \Isom{k}{p}} \lambdamax{\mat{W}^*(-\mat{X})\mat{W}}
\geq \lambdamax{\mat{W}_-^*(-\mat{A})\mat{W}_-} + t} \\
& \leq \Prob{\lambdamax{\mat{W}_-^*(-\mat{X})\mat{W}_-} -
\lambdamax{\mat{W}_-^*(-\mat{A})\mat{W}_-} \geq t} \\
& \leq \Prob{\lambdamax{\mat{W}_-^*(\mat{A} - \mat{X})\mat{W}_-} \geq t}. 
\end{align*}
The final inequality follows from the subadditivity of the maximum eigenvalue
mapping. 
\end{proof}

\begin{proof}[Proof of Lemma \ref{ch1:lemma:gaussiancentralsecondmoment}]
We begin by taking $\mat{S}$ to be the positive-semidefinite square root of
$\mat{G}.$ Let $\mat{S} = \mat{U} \mat{\Lambda} \mat{U}^\star$ be the eigenvalue
decomposition of $\mat{S},$ and let $\vec{\gamma}$ be a $\mathcal{N}(\vec{0},
\mathbf{I}_p)$ random variable. Recalling that $\mat{G}$ is the covariance
matrix of $\mat{\xi}$, we see that $\vec{\xi}$ and $\mat{U} \mat{\Lambda}
\vec{\gamma}$ are identically distributed. Thus,
\begin{align}
\E(\vec{\xi}\vec{\xi}^\star - \mat{G})^2 
& = \E(\mat{U} \mat{\Lambda} \vec{\gamma}\vec{\gamma}^\star \mat{\Lambda}
\mat{U}^\star - \mat{U} \mat{\Lambda}^2 \mat{U}^\star)^2 \notag \\
& = \mat{U} \mat{\Lambda} \E(\vec{\gamma}\vec{\gamma}^\star \mat{\Lambda}^2
\vec{\gamma}\vec{\gamma}^\star) \mat{\Lambda} \mat{U}^\star - \mat{G}^2. 
\label{ch1:eqn:rankonewishartmatrixvarianceexpansion}
\end{align}

Consider the $(i,j)$ entry of the matrix being averaged:
\[
\E(\vec{\gamma}\vec{\gamma}^\star \mat{\Lambda}^2
\vec{\gamma}\vec{\gamma}^\star)_{ij} =  \sum_k \E(\gamma_i \gamma_j \gamma_k^2)
\lambda_k^2.
\]
The $(i,j)$ entry of this matrix is zero because the entries of $\vec{\gamma}$
are independent and symmetric. Furthermore, the $(i,i)$ entry satisfies
\[
\E(\vec{\gamma}\vec{\gamma}^\star \mat{\Lambda}^2
\vec{\gamma}\vec{\gamma}^\star)_{ii} = \E(\gamma_i^4) \lambda_i^2 + \sum_{k \neq
i} \E(\gamma_k^2) \lambda_k^2 = 2 \lambda_i^2 + \tr(\mat{\Lambda}^2).
\]
We have shown
\[
\E(\vec{\gamma} \vec{\gamma}^\star \mat{\Lambda}^2
\vec{\gamma}\vec{\gamma}^\star) = 2 \mat{\Lambda}^2 + \tr(\mat{G}) \cdot
\mathbf{I}.
\]
This equality and \eqref{ch1:eqn:rankonewishartmatrixvarianceexpansion} imply the
desired result.
\end{proof}

%\begin{proof}[Proof of Lemma \ref{ch1:lemma:gaussiancentralsecondmoment}]
%We begin by taking $\mat{G}$ to be the positive-semidefinite square root of
% $\mat{C}.$ Let $\vec{\gamma}$ be a $\mathcal{N}(\vec{0},\mathbf{I})$ random
% variable. Recalling that $\vec{\xi} \sim \mathcal{N}(\vec{0}, \mat{C}),$ we see
% that $\vec{\xi}$ and $\mat{G}\vec{\gamma}$ are identically distributed, so 
%\begin{align*} 
%\E(\vec{\xi}\vec{\xi}^\star - \mat{C})^2 & 
%= \E(\mat{G}(\vec{\gamma}\vec{\gamma}^\star - \mathbf{I})\mat{G})^2 
%= \mat{G} \E (\vec{\gamma}\vec{\gamma}^\star \mat{G}^2
% \vec{\gamma}\vec{\gamma}^\star) \mat{G} - \mat{G}^4. 
%\end{align*}
%Next, consider the $(i,j)$ entry of the matrix being averaged:
%$$
%(\vec{\gamma}\vec{\gamma}^\star \mat{G}^2 \vec{\gamma}\vec{\gamma}^\star)_{ij}
% = \sum_{kl} \gamma_i \gamma_j \gamma_k \gamma_l (\mat{G}^2)_{ij}. 
%$$
%Using the properties of standard Gaussian variables,
%$$
%\E (\gamma_i \gamma_j \gamma_k \gamma_l) = 
%\begin{cases}
%1 & \text{ if } i=j, k=l, i\neq k \\
%1 & \text{ if } i=k, j=l, i \neq j \\
%1 & \text{ if } i=l, j=k, i \neq j \\
%3 & \text{ if } i= j = k= l \\
%0 & \text{ otherwise, }
%\end{cases}
%$$
%so the $(i,j)$ entry of the matrix under consideration is
%$$
%(\vec{\gamma}\vec{\gamma}^\star \mat{G}^2 \vec{\gamma}\vec{\gamma}^\star)_{ij}
% = \delta_{ij} \left[ 3 (\mat{G}^2)_{ij} + \sum_{k \neq i} (\mat{G}^2)_{kk}
% \right] + (1 - \delta_{ij})[ (\mat{G}^2)_{ij} + (\mat{G}^2)_{ji} ] = 2
% (\mat{G}^2)_{ij} + \tr \mat{G}^2.
%$$
%Therefore, 
%\begin{align*} 
%\E(\vec{\xi}\vec{\xi}^\star - \mat{C})^2 
%= \mat{G} \E (\vec{\gamma}\vec{\gamma}^\star \mat{G}^2
% \vec{\gamma}\vec{\gamma}^\star) \mat{G} - \mat{G}^4 = \mat{C}^2 + \tr(\mat{C})
% \mat{C}.
%\end{align*}
%This is the desired expression for the matrix variance of a rank-one Wishart
% matrix.
%\end{proof}

\begin{proof}[Proof of Lemma \ref{ch1:lemma:momentbound}]

Factor the covariance matrix of $\vec{\xi}$ as $\mat{G} = \mat{U\Lambda
U}^\star$ where $\mat{U}$ is orthogonal and $\mat{\Lambda} =
\text{diag}(\lambda_1, \ldots, \lambda_p)$ is the matrix of eigenvalues of
$\mat{G}$. Let $\vec{\gamma}$ be a $\mathcal{N}(\vec{0},\mathbf{I}_p)$ random
variable. Then $\vec{\xi}$ and $\mat{U\Lambda}^{1/2} \vec{\gamma}$ are
identically distributed, so 
\begin{align} 
\E (\vec{\xi}\vec{\xi}^\star)^m & = \E\left[(\vec{\xi}^\star\vec{\xi})^{m-1}
\vec{\xi}\vec{\xi}^\star \right] = \E\left[ (\vec{\gamma}^\star \mat{\Lambda}
\vec{\gamma})^{m-1} \mat{U\Lambda}^{1/2} \vec{\gamma}\vec{\gamma}^\star
\mat{\Lambda}^{1/2} \mat{U}^\star \right]\notag \\
 & = \mat{U\Lambda}^{1/2} \E \left[ (\vec{\gamma}^\star \mat{\Lambda}
\vec{\gamma})^{m-1} \vec{\gamma}\vec{\gamma}^\star \right]
\mat{\Lambda}^{1/2}\mat{U}^\star. 
\label{ch1:eqn:rankonewishartpowers}
\end{align}
Consider the $(i,j)$ entry of the bracketed matrix in
\eqref{ch1:eqn:rankonewishartpowers}:
\begin{equation}
\E\left[(\vec{\gamma}^\star \mat{\Lambda} \vec{\gamma})^{m-1} \gamma_i\gamma_j
\right] = \E\left[\left(\sum\nolimits_{\ell=1}^p \lambda_\ell \gamma_\ell^2
\right)^{m-1} \gamma_i \gamma_j\right]. 
\label{ch1:eqn:gaussianchaos}
\end{equation}
From this expression, and the independence and symmetry of the Gaussian variables $\{
\gamma_i \},$ we see that this matrix is diagonal.

To bound the diagonal entries, use a multinomial expansion to further develop
the sum in \eqref{ch1:eqn:gaussianchaos} for the $(i,i)$ entry:
\[
 \E\left[(\vec{\gamma}^\star \mat{\Lambda} \vec{\gamma})^{m-1} \gamma_i^2
\right]  
= \sum_{\ell_1 + \cdots +\ell_p = m-1} \binom{m-1}{\ell_1, \ldots, \ell_p}
\lambda_1^{\ell_1} \cdots \lambda_p^{\ell_p} \E\left[\gamma_1^{2\ell_1}
\cdots \gamma_p^{2\ell_p} \gamma_i^2 \right].
\]
Now we use the generalized AM--GM inequality to replace the expectation of the
product of Gaussians with the $2m$th moment of a single standard Gaussian $g$. 
Denote the $L_r$ norm of a random variable $X$ by
\[ 
  \lnorm{r}{X} = \left(\E |X|^r \right)^{1/r}. 
\]
Since $\ell_1, \ldots, \ell_p$ are nonnegative integers summing to $m-1$, the
generalized AM-GM inequality justifies the first of the following inequalities:
\begin{align*}
\E \gamma_1^{2\ell_1} \cdots \gamma_p^{2\ell_p} \gamma_i^2 & \leq  \E
\left(\frac{\ell_1 |\gamma_1| + \cdots + \ell_p |\gamma_p| +
|\gamma_i|}{m}\right)^{2m} =
 \lnorm{2m}{ \frac{1}{m} \left(|\gamma_i| + \sum_{j=1}^p \ell_j |\gamma_j|
\right) }^{2m} \\
 & \leq \left( \frac{1}{m} \left( \lnorm{2m}{\gamma_i} + \sum_{j=1}^p \ell_j
\lnorm{2m}{\gamma_j} \right) \right)^{2m} \\
 & = \left(\frac{1 + \ell_1 + \ldots + \ell_p}{m} \right)^{2m}
\lnorm{2m}{g}^{2m} = \E (g^{2m}).  
\end{align*}
The second inequality is the triangle inequality for $L_r$ norms. Now we reverse
the multinomial expansion to see that the diagonal terms satisfy the inequality
\begin{align}
\E\left[(\vec{\gamma}^\star \mat{\Lambda} \vec{\gamma})^{m-1}\gamma_i^2\right] &
\leq \sum_{\ell_1 + \cdots + \ell_p = m-1} \binom{m-1}{\ell_1, \ldots,
\ell_p } \lambda_1^{\ell_1} \cdots \lambda_p^{\ell_p} \E (g^{2m}) \notag
\\
&  = (\lambda_1 + \ldots + \lambda_p)^{m-1}\E ( g^{2m}) = \tr(\mat{G})^{m-1} \E
(g^{2m}). 
\label{ch1:eqn:diagonalest}
\end{align}
Estimate $\E(g^{2m})$ using the fact that $\Gamma(x)$ is increasing for $x \geq
1:$
\begin{align*}
\E \left( g^{2m} \right) & = \frac{2^m}{\sqrt{\pi}} \Gamma(m + 1/2) <
\frac{2^m}{\sqrt{\pi}} \Gamma(m+1) = \frac{2^m}{\sqrt{\pi}} m! \quad \text{for }
m \geq 1.
\end{align*}
Combine this result with \eqref{ch1:eqn:diagonalest} to see that
\[
\E\left[(\vec{\gamma}^\star \mat{\Lambda}
\vec{\gamma})^{m-1}\vec{\gamma}\vec{\gamma}^\star \right] \preceq
\frac{2^m}{\sqrt{\pi}} m! \tr(\mat{G})^{m-1} \cdot \mathbf{I}. 
\] 
Complete the proof by using this estimate in \eqref{ch1:eqn:rankonewishartpowers}.
\end{proof}

% \begin{lemma}
% \label{lemma:bernsteinmomentbound}
% Let $\mat{X}$ be a random $n \times n$ Hermitian matrix such that for some
% positive semidefinite $\mat{\sigma}^2$ and some constant $M \in (0, \infty),$
% \begin{equation}
% \E \mat{X}^k \preceq \frac{k!}{2} \mat{\sigma}^2 M^{k-2} \quad \text{for } k
% \geq 2. 
% \label{eqn:bernsteinmomentbound}
% \end{equation}
% Then 
% $$
% \E \exp(\theta \mat{X}) \preceq \exp\left(\theta \E\mat{X} + \frac{\theta^2
% \mat{\sigma}^2}{2-2M\theta}\right) \quad \text{for } 0 \leq M\theta < 1.
% $$
% \end{lemma}